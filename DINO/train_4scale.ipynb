{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n!git clone https://github.com/jingwu2001/DINO.git\nos.chdir(\"DINO\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!bash scripts/download.sh","metadata":{"id":"amgZXCFZtuxV","outputId":"8a0387ed-c6dc-4118-a55b-b17e69c1c2f2","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:35:09.644073Z","iopub.execute_input":"2025-12-09T12:35:09.644421Z","iopub.status.idle":"2025-12-09T12:36:33.702353Z","shell.execute_reply.started":"2025-12-09T12:35:09.644390Z","shell.execute_reply":"2025-12-09T12:36:33.701675Z"}},"outputs":[{"name":"stdout","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1IAnSJt2-NhWtOMugM7np0emBqs2IrYsj\nFrom (redirected): https://drive.google.com/uc?id=1IAnSJt2-NhWtOMugM7np0emBqs2IrYsj&confirm=t&uuid=e102dbbe-0252-40f8-a77b-f3d7dd1119f7\nTo: /kaggle/working/DINO/data/test_image.zip\n100%|██████████████████████████████████████| 1.83G/1.83G [00:20<00:00, 90.5MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=15WD2-haLeGe2JQeVe4iS4TUwoF6R-waO\nFrom (redirected): https://drive.google.com/uc?id=15WD2-haLeGe2JQeVe4iS4TUwoF6R-waO&confirm=t&uuid=96166753-15c9-4012-ac57-b3cb7f9950dc\nTo: /kaggle/working/DINO/data/train13.zip\n100%|██████████████████████████████████████| 1.09G/1.09G [00:19<00:00, 55.9MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1rwxLKn4MAy0pUYKEWwvBy8-v2XazD66S\nFrom (redirected): https://drive.google.com/uc?id=1rwxLKn4MAy0pUYKEWwvBy8-v2XazD66S&confirm=t&uuid=e4a86a06-ec5e-4b80-a34c-78f768f50191\nTo: /kaggle/working/DINO/data/val15.zip\n100%|█████████████████████████████████████████| 191M/191M [00:00<00:00, 205MB/s]\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1AwUn5EebmmLBo7njjW_Ng1q9zDrqkNbB\nFrom (redirected): https://drive.google.com/uc?id=1AwUn5EebmmLBo7njjW_Ng1q9zDrqkNbB&confirm=t&uuid=0505ff4b-4a7f-444a-97f2-84e28af84c13\nTo: /kaggle/working/DINO/ckpt/checkpoint0033_4scale.pth\n100%|████████████████████████████████████████| 562M/562M [00:06<00:00, 88.4MB/s]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"os.listdir('data')","metadata":{"id":"sS7NZElr7ipp","outputId":"fd43a667-5658-4c65-ee74-54e64e1cdd52","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:36:33.703314Z","iopub.execute_input":"2025-12-09T12:36:33.703582Z","iopub.status.idle":"2025-12-09T12:36:33.709294Z","shell.execute_reply.started":"2025-12-09T12:36:33.703556Z","shell.execute_reply":"2025-12-09T12:36:33.708838Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['train2017',\n 'test_image.zip',\n 'val15.zip',\n 'testing_image',\n 'val2017',\n 'train13.zip']"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# os.rename('data/training_image', 'data/train2017')\n!python tools/prepare_valve_coco.py","metadata":{"id":"rWZQGJcc6XSB","outputId":"4bf1e95c-1c06-4776-c391-d003946a43cf","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:36:33.710158Z","iopub.execute_input":"2025-12-09T12:36:33.710455Z","iopub.status.idle":"2025-12-09T12:36:36.998409Z","shell.execute_reply.started":"2025-12-09T12:36:33.710432Z","shell.execute_reply":"2025-12-09T12:36:36.997774Z"}},"outputs":[{"name":"stdout","text":"Processing Training Set...\nScanning dataset in data/train2017...\n100%|███████████████████████████████████| 9972/9972 [00:00<00:00, 110513.49it/s]\nFound 9972 images, 2493 positives.\nGenerating train.json...\n100%|███████████████████████████████████| 9972/9972 [00:00<00:00, 127070.77it/s]\nSaved to data/annotations/instances_train2017.json\nProcessing Validation Set...\nScanning dataset in data/val2017...\n100%|███████████████████████████████████| 1764/1764 [00:00<00:00, 111549.63it/s]\nFound 1764 images, 294 positives.\nGenerating val.json...\n100%|███████████████████████████████████| 1764/1764 [00:00<00:00, 194745.01it/s]\nSaved to data/annotations/instances_val2017.json\nDone!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install -r requirements.txt","metadata":{"id":"hCJu1CtTxotl","outputId":"c601ac30-ed86-46fc-a11f-77917d32f8ea","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:36:36.999482Z","iopub.execute_input":"2025-12-09T12:36:36.999760Z","iopub.status.idle":"2025-12-09T12:37:55.649872Z","shell.execute_reply.started":"2025-12-09T12:36:36.999733Z","shell.execute_reply":"2025-12-09T12:37:55.649138Z"}},"outputs":[{"name":"stdout","text":"Collecting pycocotools (from -r requirements.txt (line 2))\n  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-install-hrzr5ug3/pycocotools_0b6d23f2e3ec4323b67dfd8496c96550\n  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-install-hrzr5ug3/pycocotools_0b6d23f2e3ec4323b67dfd8496c96550\n  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting panopticapi (from -r requirements.txt (line 6))\n  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-install-hrzr5ug3/panopticapi_9d3d4bbaf7534682b262f991fe6d5f9d\n  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-install-hrzr5ug3/panopticapi_9d3d4bbaf7534682b262f991fe6d5f9d\n  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (3.0.12)\nCollecting submitit (from -r requirements.txt (line 3))\n  Downloading submitit-1.5.3-py3-none-any.whl.metadata (7.9 kB)\nRequirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (2.6.0+cu124)\nRequirement already satisfied: torchvision>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.21.0+cu124)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (1.15.3)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (3.1.0)\nCollecting addict (from -r requirements.txt (line 9))\n  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nCollecting yapf (from -r requirements.txt (line 10))\n  Downloading yapf-0.43.0-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.0.19)\nRequirement already satisfied: Pillow>=10.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (11.3.0)\nRequirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools->-r requirements.txt (line 2)) (75.2.0)\nRequirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools->-r requirements.txt (line 2)) (3.7.2)\nRequirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 3)) (3.1.2)\nRequirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.11/dist-packages (from submitit->-r requirements.txt (line 3)) (4.15.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.20.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.5.0->-r requirements.txt (line 4))\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.5.0->-r requirements.txt (line 4))\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.5.0->-r requirements.txt (line 4))\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.5.0->-r requirements.txt (line 4))\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.5.0->-r requirements.txt (line 4))\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.5.0->-r requirements.txt (line 4))\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.5.0->-r requirements.txt (line 4))\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.5.0->-r requirements.txt (line 4))\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.5.0->-r requirements.txt (line 4))\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.5.0->-r requirements.txt (line 4))\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->-r requirements.txt (line 4)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->-r requirements.txt (line 4)) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.6.0->-r requirements.txt (line 5)) (1.26.4)\nRequirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.11/dist-packages (from yapf->-r requirements.txt (line 10)) (4.5.0)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm->-r requirements.txt (line 11)) (6.0.3)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm->-r requirements.txt (line 11)) (0.36.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->-r requirements.txt (line 11)) (0.5.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (4.59.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (2.4.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 11)) (2.32.5)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 11)) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm->-r requirements.txt (line 11)) (1.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->-r requirements.txt (line 4)) (3.0.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools->-r requirements.txt (line 2)) (1.17.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm->-r requirements.txt (line 11)) (2025.10.5)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision>=0.6.0->-r requirements.txt (line 5)) (2024.2.0)\nDownloading submitit-1.5.3-py3-none-any.whl (75 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.5/75.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nDownloading yapf-0.43.0-py3-none-any.whl (256 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pycocotools, panopticapi\n  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pycocotools: filename=pycocotools-2.0-cp311-cp311-linux_x86_64.whl size=399222 sha256=1d71b076aabffe7f8e6c3141ebdab4c435bdf292ab44b1abd988b44db498f390\n  Stored in directory: /tmp/pip-ephem-wheel-cache-71tblbnl/wheels/6d/69/75/358c50a37672dfda8d74ba3b30ec49fb75d52f7c081886d503\n  Building wheel for panopticapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for panopticapi: filename=panopticapi-0.1-py3-none-any.whl size=8259 sha256=0b6df0e439ccf23dbe223dbd59879d80f61790b9f0794ff931870dc3c3fc76c9\n  Stored in directory: /tmp/pip-ephem-wheel-cache-71tblbnl/wheels/1e/dc/23/d70628297e507c01e9be79a815856549c351a79f86a1af064d\nSuccessfully built pycocotools panopticapi\nInstalling collected packages: addict, yapf, submitit, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pycocotools, panopticapi\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: pycocotools\n    Found existing installation: pycocotools 2.0.10\n    Uninstalling pycocotools-2.0.10:\n      Successfully uninstalled pycocotools-2.0.10\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed addict-2.4.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 panopticapi-0.1 pycocotools-2.0 submitit-1.5.3 yapf-0.43.0\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!bash scripts/setup.sh","metadata":{"id":"glCh1Njgzat3","outputId":"7fee8755-8a57-49ab-88fe-e76d5918bc39","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:37:55.650935Z","iopub.execute_input":"2025-12-09T12:37:55.651224Z","iopub.status.idle":"2025-12-09T12:40:01.292953Z","shell.execute_reply.started":"2025-12-09T12:37:55.651195Z","shell.execute_reply":"2025-12-09T12:40:01.292236Z"}},"outputs":[{"name":"stdout","text":"running build\nrunning build_py\ncreating build/lib.linux-x86_64-cpython-311/modules\ncopying modules/ms_deform_attn.py -> build/lib.linux-x86_64-cpython-311/modules\ncopying modules/__init__.py -> build/lib.linux-x86_64-cpython-311/modules\ncreating build/lib.linux-x86_64-cpython-311/functions\ncopying functions/__init__.py -> build/lib.linux-x86_64-cpython-311/functions\ncopying functions/ms_deform_attn_func.py -> build/lib.linux-x86_64-cpython-311/functions\nrunning build_ext\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:448: UserWarning: The detected CUDA version (12.5) has a minor version mismatch with the version that was used to compile PyTorch (12.4). Most likely this shouldn't be a problem.\n  warnings.warn(CUDA_MISMATCH_WARN.format(cuda_str_version, torch.version.cuda))\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:458: UserWarning: There are no x86_64-linux-gnu-g++ version bounds defined for CUDA version 12.5\n  warnings.warn(f'There are no {compiler_name} version bounds defined for CUDA version {cuda_str_version}')\nbuilding 'MultiScaleDeformableAttention' extension\ncreating /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/kaggle/working/DINO/models/dino/ops/src/cpu\ncreating /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/kaggle/working/DINO/models/dino/ops/src/cuda\n/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nIf this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n  warnings.warn(\nEmitting ninja build file /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/build.ninja...\nCompiling objects...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n[1/3] c++ -MMD -MF /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/kaggle/working/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DWITH_CUDA -I/kaggle/working/DINO/models/dino/ops/src -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.cpp -o /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/kaggle/working/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.o.d -DWITH_CUDA -I/kaggle/working/DINO/models/dino/ops/src -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu -o /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options ''\"'\"'-fPIC'\"'\"'' -DCUDA_HAS_FP16=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 -std=c++17\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(261): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_im2col_cuda(cudaStream_t, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *) [with scalar_t=double]\" at line 64 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(762): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(872): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(331): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(436): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(544): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_im2col_cuda.cuh(649): warning #177-D: variable \"q_col\" was declared but never referenced\n      const int q_col = _temp % num_query;\n                ^\n          detected during instantiation of \"void ms_deformable_col2im_cuda(cudaStream_t, const scalar_t *, const scalar_t *, const int64_t *, const int64_t *, const scalar_t *, const scalar_t *, int, int, int, int, int, int, int, scalar_t *, scalar_t *, scalar_t *) [with scalar_t=double]\" at line 134 of /kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu\n\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In function ‘at::Tensor ms_deform_attn_cuda_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)’:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:35:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   35 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~~~^~\n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:36:73: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   36 |     AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~~~~~~^~\n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:37:68: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   37 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~^~\n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:38:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   38 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~^~\n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In lambda function:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:973: warning: ‘T* at::Tensor::data() const [with T = double]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1059: warning: ‘T* at::Tensor::data() const [with T = long int]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1102: warning: ‘T* at::Tensor::data() const [with T = long int]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1135: warning: ‘T* at::Tensor::data() const [with T = double]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1218: warning: ‘T* at::Tensor::data() const [with T = double]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:1376: warning: ‘T* at::Tensor::data() const [with T = double]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In lambda function:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2159: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2245: warning: ‘T* at::Tensor::data() const [with T = long int]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2288: warning: ‘T* at::Tensor::data() const [with T = long int]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2320: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2402: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:64:2559: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n   64 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_forward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In function ‘std::vector<at::Tensor> ms_deform_attn_cuda_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)’:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:101:70: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  101 |     AT_ASSERTM(spatial_shapes.type().is_cuda(), \"spatial_shapes must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~~~^~\n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:102:73: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  102 |     AT_ASSERTM(level_start_index.type().is_cuda(), \"level_start_index must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~~~~~~^~\n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:103:68: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  103 |     AT_ASSERTM(sampling_loc.type().is_cuda(), \"sampling_loc must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~~^~\n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:104:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  104 |     AT_ASSERTM(attn_weight.type().is_cuda(), \"attn_weight must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~^~\n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:105:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n  105 |     AT_ASSERTM(grad_output.type().is_cuda(), \"grad_output must be a CUDA tensor\");\n      |                                                   ~~~~~~~~~~~~~~~~^~\n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:1: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In lambda function:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:983: warning: ‘T* at::Tensor::data() const [with T = double]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1009: warning: ‘T* at::Tensor::data() const [with T = double]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1095: warning: ‘T* at::Tensor::data() const [with T = long int]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1138: warning: ‘T* at::Tensor::data() const [with T = long int]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1171: warning: ‘T* at::Tensor::data() const [with T = double]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1254: warning: ‘T* at::Tensor::data() const [with T = double]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1415: warning: ‘T* at::Tensor::data() const [with T = double]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1499: warning: ‘T* at::Tensor::data() const [with T = double]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:1587: warning: ‘T* at::Tensor::data() const [with T = double]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu: In lambda function:\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2431: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2456: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2542: warning: ‘T* at::Tensor::data() const [with T = long int]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2585: warning: ‘T* at::Tensor::data() const [with T = long int]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2617: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2699: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2859: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:2942: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.cu:134:3029: warning: ‘T* at::Tensor::data() const [with T = float]’ is deprecated: Tensor.data<T>() is deprecated. Please use Tensor.data_ptr<T>() instead. [-Wdeprecated-declarations]\n  134 |         AT_DISPATCH_FLOATING_TYPES(value.scalar_type(), \"ms_deform_attn_backward_cuda\", ([&] {\n      |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^ \n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:247:1: note: declared here\n  247 |   T * data() const {\n      | ^ ~~\n[3/3] c++ -MMD -MF /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/kaggle/working/DINO/models/dino/ops/src/vision.o.d -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DWITH_CUDA -I/kaggle/working/DINO/models/dino/ops/src -I/usr/local/lib/python3.11/dist-packages/torch/include -I/usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.11/dist-packages/torch/include/TH -I/usr/local/lib/python3.11/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.11 -c -c /kaggle/working/DINO/models/dino/ops/src/vision.cpp -o /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/kaggle/working/DINO/models/dino/ops/src/vision.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=MultiScaleDeformableAttention -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\nIn file included from /kaggle/working/DINO/models/dino/ops/src/vision.cpp:11:\n/kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h: In function ‘at::Tensor ms_deform_attn_forward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)’:\n/kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h:29:19: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   29 |     if (value.type().is_cuda())\n      |         ~~~~~~~~~~^~\nIn file included from /usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/Tensor.h:3,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/ATen/Tensor.h:3,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:5,\n                 from /kaggle/working/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.h:12,\n                 from /kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h:13,\n                 from /kaggle/working/DINO/models/dino/ops/src/vision.cpp:11:\n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      |                              ^~~~\nIn file included from /kaggle/working/DINO/models/dino/ops/src/vision.cpp:11:\n/kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h: In function ‘std::vector<at::Tensor> ms_deform_attn_backward(const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, const at::Tensor&, int)’:\n/kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h:51:19: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n   51 |     if (value.type().is_cuda())\n      |         ~~~~~~~~~~^~\nIn file included from /usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/Tensor.h:3,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/ATen/Tensor.h:3,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/variable.h:6,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n                 from /usr/local/lib/python3.11/dist-packages/torch/include/torch/extension.h:5,\n                 from /kaggle/working/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.h:12,\n                 from /kaggle/working/DINO/models/dino/ops/src/ms_deform_attn.h:13,\n                 from /kaggle/working/DINO/models/dino/ops/src/vision.cpp:11:\n/usr/local/lib/python3.11/dist-packages/torch/include/ATen/core/TensorBody.h:225:30: note: declared here\n  225 |   DeprecatedTypeProperties & type() const {\n      |                              ^~~~\nx86_64-linux-gnu-g++ -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/kaggle/working/DINO/models/dino/ops/src/cpu/ms_deform_attn_cpu.o /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/kaggle/working/DINO/models/dino/ops/src/cuda/ms_deform_attn_cuda.o /kaggle/working/DINO/models/dino/ops/build/temp.linux-x86_64-cpython-311/kaggle/working/DINO/models/dino/ops/src/vision.o -L/usr/local/lib/python3.11/dist-packages/torch/lib -L/usr/local/cuda/lib64 -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-cpython-311/MultiScaleDeformableAttention.cpython-311-x86_64-linux-gnu.so\nrunning install\n/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` directly.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\n/usr/local/lib/python3.11/dist-packages/setuptools/_distutils/cmd.py:66: EasyInstallDeprecationWarning: easy_install command is deprecated.\n!!\n\n        ********************************************************************************\n        Please avoid running ``setup.py`` and ``easy_install``.\n        Instead, use pypa/build, pypa/installer or other\n        standards-based tools.\n\n        See https://github.com/pypa/setuptools/issues/917 for details.\n        ********************************************************************************\n\n!!\n  self.initialize_options()\nrunning bdist_egg\nrunning egg_info\ncreating MultiScaleDeformableAttention.egg-info\nwriting MultiScaleDeformableAttention.egg-info/PKG-INFO\nwriting dependency_links to MultiScaleDeformableAttention.egg-info/dependency_links.txt\nwriting top-level names to MultiScaleDeformableAttention.egg-info/top_level.txt\nwriting manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\nwriting manifest file 'MultiScaleDeformableAttention.egg-info/SOURCES.txt'\ninstalling library code to build/bdist.linux-x86_64/egg\nrunning install_lib\ncreating build/bdist.linux-x86_64/egg\ncreating build/bdist.linux-x86_64/egg/modules\ncopying build/lib.linux-x86_64-cpython-311/modules/ms_deform_attn.py -> build/bdist.linux-x86_64/egg/modules\ncopying build/lib.linux-x86_64-cpython-311/modules/__init__.py -> build/bdist.linux-x86_64/egg/modules\ncopying build/lib.linux-x86_64-cpython-311/MultiScaleDeformableAttention.cpython-311-x86_64-linux-gnu.so -> build/bdist.linux-x86_64/egg\ncreating build/bdist.linux-x86_64/egg/functions\ncopying build/lib.linux-x86_64-cpython-311/functions/__init__.py -> build/bdist.linux-x86_64/egg/functions\ncopying build/lib.linux-x86_64-cpython-311/functions/ms_deform_attn_func.py -> build/bdist.linux-x86_64/egg/functions\nbyte-compiling build/bdist.linux-x86_64/egg/modules/ms_deform_attn.py to ms_deform_attn.cpython-311.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/modules/__init__.py to __init__.cpython-311.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/functions/__init__.py to __init__.cpython-311.pyc\nbyte-compiling build/bdist.linux-x86_64/egg/functions/ms_deform_attn_func.py to ms_deform_attn_func.cpython-311.pyc\ncreating stub loader for MultiScaleDeformableAttention.cpython-311-x86_64-linux-gnu.so\nbyte-compiling build/bdist.linux-x86_64/egg/MultiScaleDeformableAttention.py to MultiScaleDeformableAttention.cpython-311.pyc\ncreating build/bdist.linux-x86_64/egg/EGG-INFO\ncopying MultiScaleDeformableAttention.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying MultiScaleDeformableAttention.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying MultiScaleDeformableAttention.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\ncopying MultiScaleDeformableAttention.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\nwriting build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\nzip_safe flag not set; analyzing archive contents...\n__pycache__.MultiScaleDeformableAttention.cpython-311: module references __file__\ncreating dist\ncreating 'dist/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\nremoving 'build/bdist.linux-x86_64/egg' (and everything under it)\nProcessing MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg\ncreating /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg\nExtracting MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg to /usr/local/lib/python3.11/dist-packages\nAdding MultiScaleDeformableAttention 1.0 to easy-install.pth file\n\nInstalled /usr/local/lib/python3.11/dist-packages/MultiScaleDeformableAttention-1.0-py3.11-linux-x86_64.egg\nProcessing dependencies for MultiScaleDeformableAttention==1.0\nFinished processing dependencies for MultiScaleDeformableAttention==1.0\n* True check_forward_equal_with_pytorch_double: max_abs_err 8.67e-19 max_rel_err 2.35e-16\n* True check_forward_equal_with_pytorch_float: max_abs_err 4.66e-10 max_rel_err 1.13e-07\n* True check_gradient_numerical(D=30)\n* True check_gradient_numerical(D=32)\n* True check_gradient_numerical(D=64)\n* True check_gradient_numerical(D=71)\n* True check_gradient_numerical(D=1025)\nTraceback (most recent call last):\n  File \"/kaggle/working/DINO/models/dino/ops/test.py\", line 86, in <module>\n    check_gradient_numerical(channels, True, True, True)\n  File \"/kaggle/working/DINO/models/dino/ops/test.py\", line 76, in check_gradient_numerical\n    gradok = gradcheck(func, (value.double(), shapes, level_start_index, sampling_locations.double(), attention_weights.double(), im2col_step))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\", line 2055, in gradcheck\n    return _gradcheck_helper(**args)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\", line 2084, in _gradcheck_helper\n    _gradcheck_real_imag(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\", line 1494, in _gradcheck_real_imag\n    gradcheck_fn(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\", line 1601, in _slow_gradcheck\n    _get_numerical_jacobian(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\", line 299, in _get_numerical_jacobian\n    get_numerical_jacobian_wrt_specific_input(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\", line 491, in get_numerical_jacobian_wrt_specific_input\n    return _combine_jacobian_cols(jacobian_cols, outputs, input, input.numel())\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\", line 419, in _combine_jacobian_cols\n    jacobians = _allocate_jacobians_with_outputs(\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\", line 74, in _allocate_jacobians_with_outputs\n    out: List[torch.Tensor] = [\n                              ^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/gradcheck.py\", line 75, in <listcomp>\n    t.new_zeros((numel_input, t.numel()), **options)\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 7.50 GiB. GPU 0 has a total capacity of 14.74 GiB of which 7.12 GiB is free. Process 6471 has 7.62 GiB memory in use. Of the allocated memory 7.50 GiB is allocated by PyTorch, and 541.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# python -m torch.distributed.launch --nproc_per_node=2 main.py \\\n#     --output_dir logs/DINO/R50-MS4-valve \\\n#     -c config/DINO/DINO_valve_5scale.py \\\n#     --coco_path data \\\n#     --options batch_size=8","metadata":{"id":"TiYGZJoZvYpe","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:40:01.295381Z","iopub.execute_input":"2025-12-09T12:40:01.295632Z","iopub.status.idle":"2025-12-09T12:40:01.299172Z","shell.execute_reply.started":"2025-12-09T12:40:01.295590Z","shell.execute_reply":"2025-12-09T12:40:01.298647Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!git pull","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:40:01.299878Z","iopub.execute_input":"2025-12-09T12:40:01.300244Z","iopub.status.idle":"2025-12-09T12:40:01.677030Z","shell.execute_reply.started":"2025-12-09T12:40:01.300217Z","shell.execute_reply":"2025-12-09T12:40:01.676313Z"}},"outputs":[{"name":"stdout","text":"Already up to date.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!python -m torch.distributed.launch --nproc_per_node=2 main.py \\\n  --output_dir logs/4scalekaggle \\\n  -c config/DINO/DINO_valve_4scale.py \\\n  --coco_path data \\\n  --pretrain_model_path ckpt/checkpoint0033_4scale.pth \\\n  --finetune_ignore label_enc.weight class_embed \\\n  --options \\\n    dn_scalar=100 \\\n    embed_init_tgt=TRUE \\\n    dn_label_coef=1.0 \\\n    dn_bbox_coef=1.0 \\\n    dn_box_noise_scale=1.0 \\\n    use_ema=True \\\n    ema_epoch=0 \\\n    epochs=24 \\\n    lr_drop=20 \\\n    batch_size=8","metadata":{"id":"2EinOXAgvdlw","outputId":"635c5974-cbe0-4d83-99aa-d0b8a69d5916","trusted":true,"execution":{"iopub.status.busy":"2025-12-09T12:40:01.677943Z","iopub.execute_input":"2025-12-09T12:40:01.678186Z","iopub.status.idle":"2025-12-09T12:42:33.650112Z","shell.execute_reply.started":"2025-12-09T12:40:01.678162Z","shell.execute_reply":"2025-12-09T12:42:33.649355Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated\nand will be removed in future. Use torchrun.\nNote that --use-env is set by default in torchrun.\nIf your script expects `--local-rank` argument to be set, please\nchange it to read from `os.environ['LOCAL_RANK']` instead. See \nhttps://pytorch.org/docs/stable/distributed.html#launch-utility for \nfurther instructions\n\n  main()\nW1209 12:40:03.402000 256 torch/distributed/run.py:792] \nW1209 12:40:03.402000 256 torch/distributed/run.py:792] *****************************************\nW1209 12:40:03.402000 256 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW1209 12:40:03.402000 256 torch/distributed/run.py:792] *****************************************\nworld size: 2, rank: 1, local rank: 1\nworld size: 2, rank: 0, local rank: 0\n{\n  \"SHELL\": \"/bin/bash\",\n  \"NV_LIBCUBLAS_VERSION\": \"12.5.3.2-1\",\n  \"NVIDIA_VISIBLE_DEVICES\": \"all\",\n  \"NV_NVML_DEV_VERSION\": \"12.5.82-1\",\n  \"NV_CUDNN_PACKAGE_NAME\": \"libcudnn9-cuda-12\",\n  \"NV_LIBNCCL_DEV_PACKAGE\": \"libnccl-dev=2.22.3-1+cuda12.5\",\n  \"NV_LIBNCCL_DEV_PACKAGE_VERSION\": \"2.22.3-1\",\n  \"VM_GCE_METADATA_HOST\": \"169.254.169.254\",\n  \"HOSTNAME\": \"e494833f8cd0\",\n  \"LANGUAGE\": \"en_US\",\n  \"KAGGLE_DATA_PROXY_TOKEN\": \"eyJhbGciOiJBMTI4S1ciLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0.cV2rcn_fCu9JUrruOJRRH8y5Ouc5AjF1LsFcf9yT0z0rZqSvMWV-ew.5JQDIDCeFTxDeaUuAnztVQ.e7L_qqY-XyTlBMbyaYoRyxqHnKYmR0q8HCyGSlDZQBq9xHFMm9-_ki0ETVeilHN1Qvfu3e4FemYlhfCnEWbPMAC_uNLUCvORA1bqr5rMApFrhEvJViUOjtuRacyFVMO_K1VflDK1O38OYyfmnShdQeb821rw-ClpK6q92OWaG7MsZ65khHO8ejFSwUZRd-_nPpkrERzsZn0BY2OWagf3KEZ8ZdHtb-iiLyIDOKXBIcsfcQzQPz_QYFUKTsA7q7ZQcQMcY_8peDHuvhY-cXaNOoswgVsTTM1a0ssJG2-NlRIZEqhgorYmOoWllQJJq30v.tDy4r6UdBtCLzMkyAt68qw\",\n  \"COLAB_TPU_1VM\": \"\",\n  \"NVIDIA_REQUIRE_CUDA\": \"cuda>=12.5 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551\",\n  \"NV_LIBCUBLAS_DEV_PACKAGE\": \"libcublas-dev-12-5=12.5.3.2-1\",\n  \"NV_NVTX_VERSION\": \"12.5.82-1\",\n  \"TF_CPP_MIN_LOG_LEVEL\": \"2\",\n  \"COLAB_JUPYTER_IP\": \"127.0.0.1\",\n  \"NV_CUDA_CUDART_DEV_VERSION\": \"12.5.82-1\",\n  \"NV_LIBCUSPARSE_VERSION\": \"12.5.1.3-1\",\n  \"KAGGLE_URL_BASE\": \"https://www.kaggle.com\",\n  \"NV_LIBNPP_VERSION\": \"12.3.0.159-1\",\n  \"NCCL_VERSION\": \"2.22.3-1\",\n  \"KAGGLE_DOCKER_IMAGE\": \"gcr.io/kaggle-gpu-images/python@sha256:7a9d2a6b13b3566aa6cc5a22447f3b3f99acf55dce44408deb90584b6d4b0dde\",\n  \"KAGGLE_KERNEL_INTEGRATIONS\": \"\",\n  \"KMP_LISTEN_PORT\": \"6000\",\n  \"TF_FORCE_GPU_ALLOW_GROWTH\": \"true\",\n  \"COLAB_HUMAN_READABLE_NODE_LOGS\": \"1\",\n  \"ENV\": \"/root/.bashrc\",\n  \"PWD\": \"/kaggle/working/DINO\",\n  \"TESSERACT_PATH\": \"/usr/bin/tesseract\",\n  \"NV_CUDNN_PACKAGE\": \"libcudnn9-cuda-12=9.2.1.18-1\",\n  \"NVIDIA_DRIVER_CAPABILITIES\": \"compute,utility\",\n  \"JPY_SESSION_NAME\": \"/kaggle/working/\",\n  \"LAST_FORCED_REBUILD\": \"20250623\",\n  \"NV_NVPROF_DEV_PACKAGE\": \"cuda-nvprof-12-5=12.5.82-1\",\n  \"NV_LIBNPP_PACKAGE\": \"libnpp-12-5=12.3.0.159-1\",\n  \"BUILD_DATE\": \"20251105-184650\",\n  \"NV_LIBNCCL_DEV_PACKAGE_NAME\": \"libnccl-dev\",\n  \"TCLLIBPATH\": \"/usr/share/tcltk/tcllib1.20\",\n  \"_\": \"/usr/local/bin/python\",\n  \"NV_LIBCUBLAS_DEV_VERSION\": \"12.5.3.2-1\",\n  \"NVIDIA_PRODUCT_NAME\": \"CUDA\",\n  \"UV_BUILD_CONSTRAINT\": \"\",\n  \"NV_LIBCUBLAS_DEV_PACKAGE_NAME\": \"libcublas-dev-12-5\",\n  \"NV_CUDA_CUDART_VERSION\": \"12.5.82-1\",\n  \"COLAB_JUPYTER_ALLOW_ORIGIN_PAT\": \"https://colab\\\\.(sandbox|research)\\\\.google\\\\.com\",\n  \"COLAB_WARMUP_DEFAULTS\": \"1\",\n  \"HOME\": \"/root\",\n  \"LANG\": \"en_US.UTF-8\",\n  \"CUDA_VERSION\": \"12.5.1\",\n  \"CLOUDSDK_CONFIG\": \"/content/.config\",\n  \"NV_LIBCUBLAS_PACKAGE\": \"libcublas-12-5=12.5.3.2-1\",\n  \"NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE\": \"cuda-nsight-compute-12-5=12.5.1-1\",\n  \"UV_SYSTEM_PYTHON\": \"true\",\n  \"COLAB_RELEASE_TAG\": \"release-colab_20250725-060057_RC00\",\n  \"PYDEVD_USE_FRAME_EVAL\": \"NO\",\n  \"KMP_TARGET_PORT\": \"9000\",\n  \"CLICOLOR\": \"1\",\n  \"UV_INSTALL_DIR\": \"/usr/local/bin\",\n  \"NV_LIBNPP_DEV_PACKAGE\": \"libnpp-dev-12-5=12.3.0.159-1\",\n  \"NV_LIBCUBLAS_PACKAGE_NAME\": \"libcublas-12-5\",\n  \"COLAB_KERNEL_MANAGER_PROXY_PORT\": \"6000\",\n  \"CLOUDSDK_PYTHON\": \"python3\",\n  \"NV_LIBNPP_DEV_VERSION\": \"12.3.0.159-1\",\n  \"JPY_PARENT_PID\": \"1\",\n  \"PYTHONPATH\": \"/kaggle/lib/kagglegym:/kaggle/lib\",\n  \"TERM\": \"xterm-color\",\n  \"NV_LIBCUSPARSE_DEV_VERSION\": \"12.5.1.3-1\",\n  \"KAGGLE_DATA_PROXY_PROJECT\": \"kaggle-161607\",\n  \"GIT_PAGER\": \"cat\",\n  \"KAGGLE_USER_SECRETS_TOKEN\": \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..r72JU2upeHLKWkSIgqiiFA.GnHWsLk38z2xso4SXuyqVcdF-n7A3UcZYKsFu7DeAFK7aRT0Sats_IGV9IKSxVZf_QqCkfGx-bIbCFBZoZj_g-93PVqWspUc7G1xDzVY6l9ZptYXy3QlEXSt76wJEa_znZVgmVHnR_1waC-Ab2OHuQ.7aDf4conFlFlOG0Nheu_MQ\",\n  \"LIBRARY_PATH\": \"/usr/local/cuda/lib64/stubs\",\n  \"NV_CUDNN_VERSION\": \"9.2.1.18-1\",\n  \"SHLVL\": \"0\",\n  \"PAGER\": \"cat\",\n  \"COLAB_LANGUAGE_SERVER_PROXY\": \"/usr/colab/bin/language_service\",\n  \"NV_CUDA_LIB_VERSION\": \"12.5.1-1\",\n  \"NVARCH\": \"x86_64\",\n  \"KAGGLE_KERNEL_RUN_TYPE\": \"Interactive\",\n  \"UV_CONSTRAINT\": \"\",\n  \"PYTHONUTF8\": \"1\",\n  \"NV_CUDNN_PACKAGE_DEV\": \"libcudnn9-dev-cuda-12=9.2.1.18-1\",\n  \"KAGGLE_DISABLE_GOOGLE_GENERATIVE_AI_INTEGRATION\": \"True\",\n  \"MPLBACKEND\": \"module://matplotlib_inline.backend_inline\",\n  \"NV_LIBNCCL_PACKAGE\": \"libnccl2=2.22.3-1+cuda12.5\",\n  \"LD_LIBRARY_PATH\": \"/usr/local/nvidia/lib:/usr/local/nvidia/lib64\",\n  \"KAGGLE_GCP_ZONE\": \"us-central1-f\",\n  \"MKL_THREADING_LAYER\": \"GNU\",\n  \"NV_CUDA_NSIGHT_COMPUTE_VERSION\": \"12.5.1-1\",\n  \"GIT_COMMIT\": \"e756e929cb98b6c8ef6e016c0c0f917536c4c871\",\n  \"NV_NVPROF_VERSION\": \"12.5.82-1\",\n  \"LC_ALL\": \"en_US.UTF-8\",\n  \"_PYVIZ_COMMS_INSTALLED\": \"1\",\n  \"CUDA_HOME\": \"/usr/local/cuda\",\n  \"COLAB_FILE_HANDLER_ADDR\": \"localhost:3453\",\n  \"KAGGLE_CONTAINER_NAME\": \"kaggle_o68qVJAKQ0Xp7i7leLeAa1crvrOQe7ryvrK4viLHI-284932270-webtier\",\n  \"PATH\": \"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\",\n  \"PYTHONUSERBASE\": \"/root/.local\",\n  \"NV_LIBNCCL_PACKAGE_NAME\": \"libnccl2\",\n  \"COLAB_DEBUG_ADAPTER_MUX_PATH\": \"/usr/local/bin/dap_multiplexer\",\n  \"NV_LIBNCCL_PACKAGE_VERSION\": \"2.22.3-1\",\n  \"KAGGLE_API_V1_TOKEN\": \"/etc/secrets/kaggle/api-v1-token\",\n  \"DEBIAN_FRONTEND\": \"noninteractive\",\n  \"KAGGLE_GRPC_DATA_PROXY_URL\": \"dp.kaggle.net:443\",\n  \"KAGGLE_DATA_PROXY_URL\": \"https://dp.kaggle.net\",\n  \"OMP_NUM_THREADS\": \"1\",\n  \"LOCAL_RANK\": \"0\",\n  \"RANK\": \"0\",\n  \"GROUP_RANK\": \"0\",\n  \"ROLE_RANK\": \"0\",\n  \"ROLE_NAME\": \"default\",\n  \"LOCAL_WORLD_SIZE\": \"2\",\n  \"WORLD_SIZE\": \"2\",\n  \"GROUP_WORLD_SIZE\": \"1\",\n  \"ROLE_WORLD_SIZE\": \"2\",\n  \"MASTER_ADDR\": \"127.0.0.1\",\n  \"MASTER_PORT\": \"29500\",\n  \"TORCHELASTIC_RESTART_COUNT\": \"0\",\n  \"TORCHELASTIC_MAX_RESTARTS\": \"0\",\n  \"TORCHELASTIC_RUN_ID\": \"none\",\n  \"TORCHELASTIC_USE_AGENT_STORE\": \"True\",\n  \"TORCH_NCCL_ASYNC_ERROR_HANDLING\": \"1\",\n  \"TORCHELASTIC_ERROR_FILE\": \"/tmp/torchelastic_jcx0uu67/none_9zf39fht/attempt_0/0/error.json\"\n}\nworld_size:2 rank:0 local_rank:0{\n  \"SHELL\": \"/bin/bash\",\n  \"NV_LIBCUBLAS_VERSION\": \"12.5.3.2-1\",\n  \"NVIDIA_VISIBLE_DEVICES\": \"all\",\n  \"NV_NVML_DEV_VERSION\": \"12.5.82-1\",\n  \"NV_CUDNN_PACKAGE_NAME\": \"libcudnn9-cuda-12\",\n  \"NV_LIBNCCL_DEV_PACKAGE\": \"libnccl-dev=2.22.3-1+cuda12.5\",\n  \"NV_LIBNCCL_DEV_PACKAGE_VERSION\": \"2.22.3-1\",\n  \"VM_GCE_METADATA_HOST\": \"169.254.169.254\",\n  \"HOSTNAME\": \"e494833f8cd0\",\n  \"LANGUAGE\": \"en_US\",\n  \"KAGGLE_DATA_PROXY_TOKEN\": \"eyJhbGciOiJBMTI4S1ciLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0.cV2rcn_fCu9JUrruOJRRH8y5Ouc5AjF1LsFcf9yT0z0rZqSvMWV-ew.5JQDIDCeFTxDeaUuAnztVQ.e7L_qqY-XyTlBMbyaYoRyxqHnKYmR0q8HCyGSlDZQBq9xHFMm9-_ki0ETVeilHN1Qvfu3e4FemYlhfCnEWbPMAC_uNLUCvORA1bqr5rMApFrhEvJViUOjtuRacyFVMO_K1VflDK1O38OYyfmnShdQeb821rw-ClpK6q92OWaG7MsZ65khHO8ejFSwUZRd-_nPpkrERzsZn0BY2OWagf3KEZ8ZdHtb-iiLyIDOKXBIcsfcQzQPz_QYFUKTsA7q7ZQcQMcY_8peDHuvhY-cXaNOoswgVsTTM1a0ssJG2-NlRIZEqhgorYmOoWllQJJq30v.tDy4r6UdBtCLzMkyAt68qw\",\n  \"COLAB_TPU_1VM\": \"\",\n  \"NVIDIA_REQUIRE_CUDA\": \"cuda>=12.5 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551\",\n  \"NV_LIBCUBLAS_DEV_PACKAGE\": \"libcublas-dev-12-5=12.5.3.2-1\",\n  \"NV_NVTX_VERSION\": \"12.5.82-1\",\n  \"TF_CPP_MIN_LOG_LEVEL\": \"2\",\n  \"COLAB_JUPYTER_IP\": \"127.0.0.1\",\n  \"NV_CUDA_CUDART_DEV_VERSION\": \"12.5.82-1\",\n  \"NV_LIBCUSPARSE_VERSION\": \"12.5.1.3-1\",\n  \"KAGGLE_URL_BASE\": \"https://www.kaggle.com\",\n  \"NV_LIBNPP_VERSION\": \"12.3.0.159-1\",\n  \"NCCL_VERSION\": \"2.22.3-1\",\n  \"KAGGLE_DOCKER_IMAGE\": \"gcr.io/kaggle-gpu-images/python@sha256:7a9d2a6b13b3566aa6cc5a22447f3b3f99acf55dce44408deb90584b6d4b0dde\",\n  \"KAGGLE_KERNEL_INTEGRATIONS\": \"\",\n  \"KMP_LISTEN_PORT\": \"6000\",\n  \"TF_FORCE_GPU_ALLOW_GROWTH\": \"true\",\n  \"COLAB_HUMAN_READABLE_NODE_LOGS\": \"1\",\n  \"ENV\": \"/root/.bashrc\",\n  \"PWD\": \"/kaggle/working/DINO\",\n  \"TESSERACT_PATH\": \"/usr/bin/tesseract\",\n  \"NV_CUDNN_PACKAGE\": \"libcudnn9-cuda-12=9.2.1.18-1\",\n  \"NVIDIA_DRIVER_CAPABILITIES\": \"compute,utility\",\n  \"JPY_SESSION_NAME\": \"/kaggle/working/\",\n  \"LAST_FORCED_REBUILD\": \"20250623\",\n  \"NV_NVPROF_DEV_PACKAGE\": \"cuda-nvprof-12-5=12.5.82-1\",\n  \"NV_LIBNPP_PACKAGE\": \"libnpp-12-5=12.3.0.159-1\",\n  \"BUILD_DATE\": \"20251105-184650\",\n  \"NV_LIBNCCL_DEV_PACKAGE_NAME\": \"libnccl-dev\",\n  \"TCLLIBPATH\": \"/usr/share/tcltk/tcllib1.20\",\n  \"_\": \"/usr/local/bin/python\",\n  \"NV_LIBCUBLAS_DEV_VERSION\": \"12.5.3.2-1\",\n  \"NVIDIA_PRODUCT_NAME\": \"CUDA\",\n  \"UV_BUILD_CONSTRAINT\": \"\",\n  \"NV_LIBCUBLAS_DEV_PACKAGE_NAME\": \"libcublas-dev-12-5\",\n  \"NV_CUDA_CUDART_VERSION\": \"12.5.82-1\",\n  \"COLAB_JUPYTER_ALLOW_ORIGIN_PAT\": \"https://colab\\\\.(sandbox|research)\\\\.google\\\\.com\",\n  \"COLAB_WARMUP_DEFAULTS\": \"1\",\n  \"HOME\": \"/root\",\n  \"LANG\": \"en_US.UTF-8\",\n  \"CUDA_VERSION\": \"12.5.1\",\n  \"CLOUDSDK_CONFIG\": \"/content/.config\",\n  \"NV_LIBCUBLAS_PACKAGE\": \"libcublas-12-5=12.5.3.2-1\",\n  \"NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE\": \"cuda-nsight-compute-12-5=12.5.1-1\",\n  \"UV_SYSTEM_PYTHON\": \"true\",\n  \"COLAB_RELEASE_TAG\": \"release-colab_20250725-060057_RC00\",\n  \"PYDEVD_USE_FRAME_EVAL\": \"NO\",\n  \"KMP_TARGET_PORT\": \"9000\",\n  \"CLICOLOR\": \"1\",\n  \"UV_INSTALL_DIR\": \"/usr/local/bin\",\n  \"NV_LIBNPP_DEV_PACKAGE\": \"libnpp-dev-12-5=12.3.0.159-1\",\n  \"NV_LIBCUBLAS_PACKAGE_NAME\": \"libcublas-12-5\",\n  \"COLAB_KERNEL_MANAGER_PROXY_PORT\": \"6000\",\n  \"CLOUDSDK_PYTHON\": \"python3\",\n  \"NV_LIBNPP_DEV_VERSION\": \"12.3.0.159-1\",\n  \"JPY_PARENT_PID\": \"1\",\n  \"PYTHONPATH\": \"/kaggle/lib/kagglegym:/kaggle/lib\",\n  \"TERM\": \"xterm-color\",\n  \"NV_LIBCUSPARSE_DEV_VERSION\": \"12.5.1.3-1\",\n  \"KAGGLE_DATA_PROXY_PROJECT\": \"kaggle-161607\",\n  \"GIT_PAGER\": \"cat\",\n  \"KAGGLE_USER_SECRETS_TOKEN\": \"eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..r72JU2upeHLKWkSIgqiiFA.GnHWsLk38z2xso4SXuyqVcdF-n7A3UcZYKsFu7DeAFK7aRT0Sats_IGV9IKSxVZf_QqCkfGx-bIbCFBZoZj_g-93PVqWspUc7G1xDzVY6l9ZptYXy3QlEXSt76wJEa_znZVgmVHnR_1waC-Ab2OHuQ.7aDf4conFlFlOG0Nheu_MQ\",\n  \"LIBRARY_PATH\": \"/usr/local/cuda/lib64/stubs\",\n  \"NV_CUDNN_VERSION\": \"9.2.1.18-1\",\n  \"SHLVL\": \"0\",\n  \"PAGER\": \"cat\",\n  \"COLAB_LANGUAGE_SERVER_PROXY\": \"/usr/colab/bin/language_service\",\n  \"NV_CUDA_LIB_VERSION\": \"12.5.1-1\",\n  \"NVARCH\": \"x86_64\",\n  \"KAGGLE_KERNEL_RUN_TYPE\": \"Interactive\",\n  \"UV_CONSTRAINT\": \"\",\n  \"PYTHONUTF8\": \"1\",\n  \"NV_CUDNN_PACKAGE_DEV\": \"libcudnn9-dev-cuda-12=9.2.1.18-1\",\n  \"KAGGLE_DISABLE_GOOGLE_GENERATIVE_AI_INTEGRATION\": \"True\",\n  \"MPLBACKEND\": \"module://matplotlib_inline.backend_inline\",\n  \"NV_LIBNCCL_PACKAGE\": \"libnccl2=2.22.3-1+cuda12.5\",\n  \"LD_LIBRARY_PATH\": \"/usr/local/nvidia/lib:/usr/local/nvidia/lib64\",\n  \"KAGGLE_GCP_ZONE\": \"us-central1-f\",\n  \"MKL_THREADING_LAYER\": \"GNU\",\n  \"NV_CUDA_NSIGHT_COMPUTE_VERSION\": \"12.5.1-1\",\n  \"GIT_COMMIT\": \"e756e929cb98b6c8ef6e016c0c0f917536c4c871\",\n  \"NV_NVPROF_VERSION\": \"12.5.82-1\",\n  \"LC_ALL\": \"en_US.UTF-8\",\n  \"_PYVIZ_COMMS_INSTALLED\": \"1\",\n  \"CUDA_HOME\": \"/usr/local/cuda\",\n  \"COLAB_FILE_HANDLER_ADDR\": \"localhost:3453\",\n  \"KAGGLE_CONTAINER_NAME\": \"kaggle_o68qVJAKQ0Xp7i7leLeAa1crvrOQe7ryvrK4viLHI-284932270-webtier\",\n  \"PATH\": \"/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\",\n  \"PYTHONUSERBASE\": \"/root/.local\",\n  \"NV_LIBNCCL_PACKAGE_NAME\": \"libnccl2\",\n  \"COLAB_DEBUG_ADAPTER_MUX_PATH\": \"/usr/local/bin/dap_multiplexer\",\n  \"NV_LIBNCCL_PACKAGE_VERSION\": \"2.22.3-1\",\n  \"KAGGLE_API_V1_TOKEN\": \"/etc/secrets/kaggle/api-v1-token\",\n  \"DEBIAN_FRONTEND\": \"noninteractive\",\n  \"KAGGLE_GRPC_DATA_PROXY_URL\": \"dp.kaggle.net:443\",\n  \"KAGGLE_DATA_PROXY_URL\": \"https://dp.kaggle.net\",\n  \"OMP_NUM_THREADS\": \"1\",\n  \"LOCAL_RANK\": \"1\",\n  \"RANK\": \"1\",\n  \"GROUP_RANK\": \"0\",\n  \"ROLE_RANK\": \"1\",\n  \"ROLE_NAME\": \"default\",\n  \"LOCAL_WORLD_SIZE\": \"2\",\n  \"WORLD_SIZE\": \"2\",\n  \"GROUP_WORLD_SIZE\": \"1\",\n  \"ROLE_WORLD_SIZE\": \"2\",\n  \"MASTER_ADDR\": \"127.0.0.1\",\n  \"MASTER_PORT\": \"29500\",\n  \"TORCHELASTIC_RESTART_COUNT\": \"0\",\n  \"TORCHELASTIC_MAX_RESTARTS\": \"0\",\n  \"TORCHELASTIC_RUN_ID\": \"none\",\n  \"TORCHELASTIC_USE_AGENT_STORE\": \"True\",\n  \"TORCH_NCCL_ASYNC_ERROR_HANDLING\": \"1\",\n  \"TORCHELASTIC_ERROR_FILE\": \"/tmp/torchelastic_jcx0uu67/none_9zf39fht/attempt_0/1/error.json\"\n}\n\nworld_size:2 rank:1 local_rank:1\n| distributed init (rank 0): env://\nBefore torch.distributed.barrier()\n| distributed init (rank 1): env://\n[rank0]:[W1209 12:40:08.600687927 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\nBefore torch.distributed.barrier()\n[rank1]:[W1209 12:40:08.604976258 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\nEnd torch.distributed.barrier()End torch.distributed.barrier()\n\nLoading config file from config/DINO/DINO_valve_4scale.py\n[12/09 12:40:09.396]: git:\n  sha: c705598470def8c8af889066e99aaa42c3905593, status: clean, branch: main\n\n[12/09 12:40:09.396]: Command: main.py --local-rank=0 --output_dir logs/4scalekaggle -c config/DINO/DINO_valve_4scale.py --coco_path data --pretrain_model_path ckpt/checkpoint0033_4scale.pth --finetune_ignore label_enc.weight class_embed --options dn_scalar=100 embed_init_tgt=TRUE dn_label_coef=1.0 dn_bbox_coef=1.0 dn_box_noise_scale=1.0 use_ema=True ema_epoch=0 epochs=24 lr_drop=20 batch_size=8\n[12/09 12:40:09.396]: Full config saved to logs/4scalekaggle/config_args_all.json\n[12/09 12:40:09.397]: world size: 2\n[12/09 12:40:09.397]: rank: 0\n[12/09 12:40:09.397]: local_rank: 0\n[12/09 12:40:09.397]: args: Namespace(config_file='config/DINO/DINO_valve_4scale.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'dn_box_noise_scale': 1.0, 'use_ema': True, 'ema_epoch': 0, 'epochs': 24, 'lr_drop': 20, 'batch_size': 8}, dataset_file='coco', coco_path='data', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/4scalekaggle', note='', device='cuda', seed=42, resume='', pretrain_model_path='ckpt/checkpoint0033_4scale.pth', finetune_ignore=['label_enc.weight', 'class_embed'], start_epoch=0, eval=False, num_workers=10, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=2, dist_url='env://', rank=0, local_rank=0, amp=False, gpu=0, distributed=True, dist_backend='nccl', data_aug_scales=[480, 512, 544, 576, 608, 640, 672], data_aug_max_size=512, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=1, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=8, weight_decay=0.0001, epochs=24, lr_drop=20, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='resnet50', use_checkpoint=False, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=2, match_unstable_error=True, use_ema=True, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)\n\nNamespace(config_file='config/DINO/DINO_valve_4scale.py', options={'dn_scalar': 100, 'embed_init_tgt': True, 'dn_label_coef': 1.0, 'dn_bbox_coef': 1.0, 'dn_box_noise_scale': 1.0, 'use_ema': True, 'ema_epoch': 0, 'epochs': 24, 'lr_drop': 20, 'batch_size': 8}, dataset_file='coco', coco_path='data', coco_panoptic_path=None, remove_difficult=False, fix_size=False, output_dir='logs/4scalekaggle', note='', device='cuda', seed=42, resume='', pretrain_model_path='ckpt/checkpoint0033_4scale.pth', finetune_ignore=['label_enc.weight', 'class_embed'], start_epoch=0, eval=False, num_workers=10, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=2, dist_url='env://', rank=0, local_rank=0, amp=False, gpu=0, distributed=True, dist_backend='nccl', data_aug_scales=[480, 512, 544, 576, 608, 640, 672], data_aug_max_size=512, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, num_classes=1, lr=0.0001, param_dict_type='default', lr_backbone=1e-05, lr_backbone_names=['backbone.0'], lr_linear_proj_names=['reference_points', 'sampling_offsets'], lr_linear_proj_mult=0.1, ddetr_lr_param=False, batch_size=8, weight_decay=0.0001, epochs=24, lr_drop=20, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[33, 45], modelname='dino', frozen_weights=None, backbone='resnet50', use_checkpoint=False, dilation=False, position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], backbone_freeze_keywords=None, enc_layers=6, dec_layers=6, unic_layers=0, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, num_feature_levels=4, enc_n_points=4, dec_n_points=4, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_type='standard', two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, transformer_activation='relu', batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=2.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=1.0, mask_loss_coef=1.0, dice_loss_coef=1.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, focal_alpha=0.25, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_bbox_embed_share=True, dec_pred_class_embed_share=True, use_dn=True, dn_number=100, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, embed_init_tgt=True, dn_labelbook_size=2, match_unstable_error=True, use_ema=True, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, dn_scalar=100, dn_label_coef=1.0, dn_bbox_coef=1.0)\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|███████████████████████████████████████| 97.8M/97.8M [00:00<00:00, 189MB/s]\n[12/09 12:40:14.826]: number of params:46601738\n[12/09 12:40:14.827]: params:\n{\n  \"module.transformer.level_embed\": 1024,\n  \"module.transformer.encoder.layers.0.self_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.encoder.layers.0.self_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.encoder.layers.0.self_attn.attention_weights.weight\": 32768,\n  \"module.transformer.encoder.layers.0.self_attn.attention_weights.bias\": 128,\n  \"module.transformer.encoder.layers.0.self_attn.value_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.0.self_attn.value_proj.bias\": 256,\n  \"module.transformer.encoder.layers.0.self_attn.output_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.0.self_attn.output_proj.bias\": 256,\n  \"module.transformer.encoder.layers.0.norm1.weight\": 256,\n  \"module.transformer.encoder.layers.0.norm1.bias\": 256,\n  \"module.transformer.encoder.layers.0.linear1.weight\": 524288,\n  \"module.transformer.encoder.layers.0.linear1.bias\": 2048,\n  \"module.transformer.encoder.layers.0.linear2.weight\": 524288,\n  \"module.transformer.encoder.layers.0.linear2.bias\": 256,\n  \"module.transformer.encoder.layers.0.norm2.weight\": 256,\n  \"module.transformer.encoder.layers.0.norm2.bias\": 256,\n  \"module.transformer.encoder.layers.1.self_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.encoder.layers.1.self_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.encoder.layers.1.self_attn.attention_weights.weight\": 32768,\n  \"module.transformer.encoder.layers.1.self_attn.attention_weights.bias\": 128,\n  \"module.transformer.encoder.layers.1.self_attn.value_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.1.self_attn.value_proj.bias\": 256,\n  \"module.transformer.encoder.layers.1.self_attn.output_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.1.self_attn.output_proj.bias\": 256,\n  \"module.transformer.encoder.layers.1.norm1.weight\": 256,\n  \"module.transformer.encoder.layers.1.norm1.bias\": 256,\n  \"module.transformer.encoder.layers.1.linear1.weight\": 524288,\n  \"module.transformer.encoder.layers.1.linear1.bias\": 2048,\n  \"module.transformer.encoder.layers.1.linear2.weight\": 524288,\n  \"module.transformer.encoder.layers.1.linear2.bias\": 256,\n  \"module.transformer.encoder.layers.1.norm2.weight\": 256,\n  \"module.transformer.encoder.layers.1.norm2.bias\": 256,\n  \"module.transformer.encoder.layers.2.self_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.encoder.layers.2.self_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.encoder.layers.2.self_attn.attention_weights.weight\": 32768,\n  \"module.transformer.encoder.layers.2.self_attn.attention_weights.bias\": 128,\n  \"module.transformer.encoder.layers.2.self_attn.value_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.2.self_attn.value_proj.bias\": 256,\n  \"module.transformer.encoder.layers.2.self_attn.output_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.2.self_attn.output_proj.bias\": 256,\n  \"module.transformer.encoder.layers.2.norm1.weight\": 256,\n  \"module.transformer.encoder.layers.2.norm1.bias\": 256,\n  \"module.transformer.encoder.layers.2.linear1.weight\": 524288,\n  \"module.transformer.encoder.layers.2.linear1.bias\": 2048,\n  \"module.transformer.encoder.layers.2.linear2.weight\": 524288,\n  \"module.transformer.encoder.layers.2.linear2.bias\": 256,\n  \"module.transformer.encoder.layers.2.norm2.weight\": 256,\n  \"module.transformer.encoder.layers.2.norm2.bias\": 256,\n  \"module.transformer.encoder.layers.3.self_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.encoder.layers.3.self_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.encoder.layers.3.self_attn.attention_weights.weight\": 32768,\n  \"module.transformer.encoder.layers.3.self_attn.attention_weights.bias\": 128,\n  \"module.transformer.encoder.layers.3.self_attn.value_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.3.self_attn.value_proj.bias\": 256,\n  \"module.transformer.encoder.layers.3.self_attn.output_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.3.self_attn.output_proj.bias\": 256,\n  \"module.transformer.encoder.layers.3.norm1.weight\": 256,\n  \"module.transformer.encoder.layers.3.norm1.bias\": 256,\n  \"module.transformer.encoder.layers.3.linear1.weight\": 524288,\n  \"module.transformer.encoder.layers.3.linear1.bias\": 2048,\n  \"module.transformer.encoder.layers.3.linear2.weight\": 524288,\n  \"module.transformer.encoder.layers.3.linear2.bias\": 256,\n  \"module.transformer.encoder.layers.3.norm2.weight\": 256,\n  \"module.transformer.encoder.layers.3.norm2.bias\": 256,\n  \"module.transformer.encoder.layers.4.self_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.encoder.layers.4.self_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.encoder.layers.4.self_attn.attention_weights.weight\": 32768,\n  \"module.transformer.encoder.layers.4.self_attn.attention_weights.bias\": 128,\n  \"module.transformer.encoder.layers.4.self_attn.value_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.4.self_attn.value_proj.bias\": 256,\n  \"module.transformer.encoder.layers.4.self_attn.output_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.4.self_attn.output_proj.bias\": 256,\n  \"module.transformer.encoder.layers.4.norm1.weight\": 256,\n  \"module.transformer.encoder.layers.4.norm1.bias\": 256,\n  \"module.transformer.encoder.layers.4.linear1.weight\": 524288,\n  \"module.transformer.encoder.layers.4.linear1.bias\": 2048,\n  \"module.transformer.encoder.layers.4.linear2.weight\": 524288,\n  \"module.transformer.encoder.layers.4.linear2.bias\": 256,\n  \"module.transformer.encoder.layers.4.norm2.weight\": 256,\n  \"module.transformer.encoder.layers.4.norm2.bias\": 256,\n  \"module.transformer.encoder.layers.5.self_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.encoder.layers.5.self_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.encoder.layers.5.self_attn.attention_weights.weight\": 32768,\n  \"module.transformer.encoder.layers.5.self_attn.attention_weights.bias\": 128,\n  \"module.transformer.encoder.layers.5.self_attn.value_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.5.self_attn.value_proj.bias\": 256,\n  \"module.transformer.encoder.layers.5.self_attn.output_proj.weight\": 65536,\n  \"module.transformer.encoder.layers.5.self_attn.output_proj.bias\": 256,\n  \"module.transformer.encoder.layers.5.norm1.weight\": 256,\n  \"module.transformer.encoder.layers.5.norm1.bias\": 256,\n  \"module.transformer.encoder.layers.5.linear1.weight\": 524288,\n  \"module.transformer.encoder.layers.5.linear1.bias\": 2048,\n  \"module.transformer.encoder.layers.5.linear2.weight\": 524288,\n  \"module.transformer.encoder.layers.5.linear2.bias\": 256,\n  \"module.transformer.encoder.layers.5.norm2.weight\": 256,\n  \"module.transformer.encoder.layers.5.norm2.bias\": 256,\n  \"module.transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.decoder.layers.0.cross_attn.attention_weights.weight\": 32768,\n  \"module.transformer.decoder.layers.0.cross_attn.attention_weights.bias\": 128,\n  \"module.transformer.decoder.layers.0.cross_attn.value_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.0.cross_attn.value_proj.bias\": 256,\n  \"module.transformer.decoder.layers.0.cross_attn.output_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.0.cross_attn.output_proj.bias\": 256,\n  \"module.transformer.decoder.layers.0.norm1.weight\": 256,\n  \"module.transformer.decoder.layers.0.norm1.bias\": 256,\n  \"module.transformer.decoder.layers.0.self_attn.in_proj_weight\": 196608,\n  \"module.transformer.decoder.layers.0.self_attn.in_proj_bias\": 768,\n  \"module.transformer.decoder.layers.0.self_attn.out_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.0.self_attn.out_proj.bias\": 256,\n  \"module.transformer.decoder.layers.0.norm2.weight\": 256,\n  \"module.transformer.decoder.layers.0.norm2.bias\": 256,\n  \"module.transformer.decoder.layers.0.linear1.weight\": 524288,\n  \"module.transformer.decoder.layers.0.linear1.bias\": 2048,\n  \"module.transformer.decoder.layers.0.linear2.weight\": 524288,\n  \"module.transformer.decoder.layers.0.linear2.bias\": 256,\n  \"module.transformer.decoder.layers.0.norm3.weight\": 256,\n  \"module.transformer.decoder.layers.0.norm3.bias\": 256,\n  \"module.transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.decoder.layers.1.cross_attn.attention_weights.weight\": 32768,\n  \"module.transformer.decoder.layers.1.cross_attn.attention_weights.bias\": 128,\n  \"module.transformer.decoder.layers.1.cross_attn.value_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.1.cross_attn.value_proj.bias\": 256,\n  \"module.transformer.decoder.layers.1.cross_attn.output_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.1.cross_attn.output_proj.bias\": 256,\n  \"module.transformer.decoder.layers.1.norm1.weight\": 256,\n  \"module.transformer.decoder.layers.1.norm1.bias\": 256,\n  \"module.transformer.decoder.layers.1.self_attn.in_proj_weight\": 196608,\n  \"module.transformer.decoder.layers.1.self_attn.in_proj_bias\": 768,\n  \"module.transformer.decoder.layers.1.self_attn.out_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.1.self_attn.out_proj.bias\": 256,\n  \"module.transformer.decoder.layers.1.norm2.weight\": 256,\n  \"module.transformer.decoder.layers.1.norm2.bias\": 256,\n  \"module.transformer.decoder.layers.1.linear1.weight\": 524288,\n  \"module.transformer.decoder.layers.1.linear1.bias\": 2048,\n  \"module.transformer.decoder.layers.1.linear2.weight\": 524288,\n  \"module.transformer.decoder.layers.1.linear2.bias\": 256,\n  \"module.transformer.decoder.layers.1.norm3.weight\": 256,\n  \"module.transformer.decoder.layers.1.norm3.bias\": 256,\n  \"module.transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.decoder.layers.2.cross_attn.attention_weights.weight\": 32768,\n  \"module.transformer.decoder.layers.2.cross_attn.attention_weights.bias\": 128,\n  \"module.transformer.decoder.layers.2.cross_attn.value_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.2.cross_attn.value_proj.bias\": 256,\n  \"module.transformer.decoder.layers.2.cross_attn.output_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.2.cross_attn.output_proj.bias\": 256,\n  \"module.transformer.decoder.layers.2.norm1.weight\": 256,\n  \"module.transformer.decoder.layers.2.norm1.bias\": 256,\n  \"module.transformer.decoder.layers.2.self_attn.in_proj_weight\": 196608,\n  \"module.transformer.decoder.layers.2.self_attn.in_proj_bias\": 768,\n  \"module.transformer.decoder.layers.2.self_attn.out_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.2.self_attn.out_proj.bias\": 256,\n  \"module.transformer.decoder.layers.2.norm2.weight\": 256,\n  \"module.transformer.decoder.layers.2.norm2.bias\": 256,\n  \"module.transformer.decoder.layers.2.linear1.weight\": 524288,\n  \"module.transformer.decoder.layers.2.linear1.bias\": 2048,\n  \"module.transformer.decoder.layers.2.linear2.weight\": 524288,\n  \"module.transformer.decoder.layers.2.linear2.bias\": 256,\n  \"module.transformer.decoder.layers.2.norm3.weight\": 256,\n  \"module.transformer.decoder.layers.2.norm3.bias\": 256,\n  \"module.transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.decoder.layers.3.cross_attn.attention_weights.weight\": 32768,\n  \"module.transformer.decoder.layers.3.cross_attn.attention_weights.bias\": 128,\n  \"module.transformer.decoder.layers.3.cross_attn.value_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.3.cross_attn.value_proj.bias\": 256,\n  \"module.transformer.decoder.layers.3.cross_attn.output_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.3.cross_attn.output_proj.bias\": 256,\n  \"module.transformer.decoder.layers.3.norm1.weight\": 256,\n  \"module.transformer.decoder.layers.3.norm1.bias\": 256,\n  \"module.transformer.decoder.layers.3.self_attn.in_proj_weight\": 196608,\n  \"module.transformer.decoder.layers.3.self_attn.in_proj_bias\": 768,\n  \"module.transformer.decoder.layers.3.self_attn.out_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.3.self_attn.out_proj.bias\": 256,\n  \"module.transformer.decoder.layers.3.norm2.weight\": 256,\n  \"module.transformer.decoder.layers.3.norm2.bias\": 256,\n  \"module.transformer.decoder.layers.3.linear1.weight\": 524288,\n  \"module.transformer.decoder.layers.3.linear1.bias\": 2048,\n  \"module.transformer.decoder.layers.3.linear2.weight\": 524288,\n  \"module.transformer.decoder.layers.3.linear2.bias\": 256,\n  \"module.transformer.decoder.layers.3.norm3.weight\": 256,\n  \"module.transformer.decoder.layers.3.norm3.bias\": 256,\n  \"module.transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.decoder.layers.4.cross_attn.attention_weights.weight\": 32768,\n  \"module.transformer.decoder.layers.4.cross_attn.attention_weights.bias\": 128,\n  \"module.transformer.decoder.layers.4.cross_attn.value_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.4.cross_attn.value_proj.bias\": 256,\n  \"module.transformer.decoder.layers.4.cross_attn.output_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.4.cross_attn.output_proj.bias\": 256,\n  \"module.transformer.decoder.layers.4.norm1.weight\": 256,\n  \"module.transformer.decoder.layers.4.norm1.bias\": 256,\n  \"module.transformer.decoder.layers.4.self_attn.in_proj_weight\": 196608,\n  \"module.transformer.decoder.layers.4.self_attn.in_proj_bias\": 768,\n  \"module.transformer.decoder.layers.4.self_attn.out_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.4.self_attn.out_proj.bias\": 256,\n  \"module.transformer.decoder.layers.4.norm2.weight\": 256,\n  \"module.transformer.decoder.layers.4.norm2.bias\": 256,\n  \"module.transformer.decoder.layers.4.linear1.weight\": 524288,\n  \"module.transformer.decoder.layers.4.linear1.bias\": 2048,\n  \"module.transformer.decoder.layers.4.linear2.weight\": 524288,\n  \"module.transformer.decoder.layers.4.linear2.bias\": 256,\n  \"module.transformer.decoder.layers.4.norm3.weight\": 256,\n  \"module.transformer.decoder.layers.4.norm3.bias\": 256,\n  \"module.transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\": 65536,\n  \"module.transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\": 256,\n  \"module.transformer.decoder.layers.5.cross_attn.attention_weights.weight\": 32768,\n  \"module.transformer.decoder.layers.5.cross_attn.attention_weights.bias\": 128,\n  \"module.transformer.decoder.layers.5.cross_attn.value_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.5.cross_attn.value_proj.bias\": 256,\n  \"module.transformer.decoder.layers.5.cross_attn.output_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.5.cross_attn.output_proj.bias\": 256,\n  \"module.transformer.decoder.layers.5.norm1.weight\": 256,\n  \"module.transformer.decoder.layers.5.norm1.bias\": 256,\n  \"module.transformer.decoder.layers.5.self_attn.in_proj_weight\": 196608,\n  \"module.transformer.decoder.layers.5.self_attn.in_proj_bias\": 768,\n  \"module.transformer.decoder.layers.5.self_attn.out_proj.weight\": 65536,\n  \"module.transformer.decoder.layers.5.self_attn.out_proj.bias\": 256,\n  \"module.transformer.decoder.layers.5.norm2.weight\": 256,\n  \"module.transformer.decoder.layers.5.norm2.bias\": 256,\n  \"module.transformer.decoder.layers.5.linear1.weight\": 524288,\n  \"module.transformer.decoder.layers.5.linear1.bias\": 2048,\n  \"module.transformer.decoder.layers.5.linear2.weight\": 524288,\n  \"module.transformer.decoder.layers.5.linear2.bias\": 256,\n  \"module.transformer.decoder.layers.5.norm3.weight\": 256,\n  \"module.transformer.decoder.layers.5.norm3.bias\": 256,\n  \"module.transformer.decoder.norm.weight\": 256,\n  \"module.transformer.decoder.norm.bias\": 256,\n  \"module.transformer.decoder.ref_point_head.layers.0.weight\": 131072,\n  \"module.transformer.decoder.ref_point_head.layers.0.bias\": 256,\n  \"module.transformer.decoder.ref_point_head.layers.1.weight\": 65536,\n  \"module.transformer.decoder.ref_point_head.layers.1.bias\": 256,\n  \"module.transformer.decoder.bbox_embed.0.layers.0.weight\": 65536,\n  \"module.transformer.decoder.bbox_embed.0.layers.0.bias\": 256,\n  \"module.transformer.decoder.bbox_embed.0.layers.1.weight\": 65536,\n  \"module.transformer.decoder.bbox_embed.0.layers.1.bias\": 256,\n  \"module.transformer.decoder.bbox_embed.0.layers.2.weight\": 1024,\n  \"module.transformer.decoder.bbox_embed.0.layers.2.bias\": 4,\n  \"module.transformer.decoder.class_embed.0.weight\": 256,\n  \"module.transformer.decoder.class_embed.0.bias\": 1,\n  \"module.transformer.tgt_embed.weight\": 230400,\n  \"module.transformer.enc_output.weight\": 65536,\n  \"module.transformer.enc_output.bias\": 256,\n  \"module.transformer.enc_output_norm.weight\": 256,\n  \"module.transformer.enc_output_norm.bias\": 256,\n  \"module.transformer.enc_out_bbox_embed.layers.0.weight\": 65536,\n  \"module.transformer.enc_out_bbox_embed.layers.0.bias\": 256,\n  \"module.transformer.enc_out_bbox_embed.layers.1.weight\": 65536,\n  \"module.transformer.enc_out_bbox_embed.layers.1.bias\": 256,\n  \"module.transformer.enc_out_bbox_embed.layers.2.weight\": 1024,\n  \"module.transformer.enc_out_bbox_embed.layers.2.bias\": 4,\n  \"module.transformer.enc_out_class_embed.weight\": 256,\n  \"module.transformer.enc_out_class_embed.bias\": 1,\n  \"module.label_enc.weight\": 768,\n  \"module.input_proj.0.0.weight\": 131072,\n  \"module.input_proj.0.0.bias\": 256,\n  \"module.input_proj.0.1.weight\": 256,\n  \"module.input_proj.0.1.bias\": 256,\n  \"module.input_proj.1.0.weight\": 262144,\n  \"module.input_proj.1.0.bias\": 256,\n  \"module.input_proj.1.1.weight\": 256,\n  \"module.input_proj.1.1.bias\": 256,\n  \"module.input_proj.2.0.weight\": 524288,\n  \"module.input_proj.2.0.bias\": 256,\n  \"module.input_proj.2.1.weight\": 256,\n  \"module.input_proj.2.1.bias\": 256,\n  \"module.input_proj.3.0.weight\": 4718592,\n  \"module.input_proj.3.0.bias\": 256,\n  \"module.input_proj.3.1.weight\": 256,\n  \"module.input_proj.3.1.bias\": 256,\n  \"module.backbone.0.body.layer2.0.conv1.weight\": 32768,\n  \"module.backbone.0.body.layer2.0.conv2.weight\": 147456,\n  \"module.backbone.0.body.layer2.0.conv3.weight\": 65536,\n  \"module.backbone.0.body.layer2.0.downsample.0.weight\": 131072,\n  \"module.backbone.0.body.layer2.1.conv1.weight\": 65536,\n  \"module.backbone.0.body.layer2.1.conv2.weight\": 147456,\n  \"module.backbone.0.body.layer2.1.conv3.weight\": 65536,\n  \"module.backbone.0.body.layer2.2.conv1.weight\": 65536,\n  \"module.backbone.0.body.layer2.2.conv2.weight\": 147456,\n  \"module.backbone.0.body.layer2.2.conv3.weight\": 65536,\n  \"module.backbone.0.body.layer2.3.conv1.weight\": 65536,\n  \"module.backbone.0.body.layer2.3.conv2.weight\": 147456,\n  \"module.backbone.0.body.layer2.3.conv3.weight\": 65536,\n  \"module.backbone.0.body.layer3.0.conv1.weight\": 131072,\n  \"module.backbone.0.body.layer3.0.conv2.weight\": 589824,\n  \"module.backbone.0.body.layer3.0.conv3.weight\": 262144,\n  \"module.backbone.0.body.layer3.0.downsample.0.weight\": 524288,\n  \"module.backbone.0.body.layer3.1.conv1.weight\": 262144,\n  \"module.backbone.0.body.layer3.1.conv2.weight\": 589824,\n  \"module.backbone.0.body.layer3.1.conv3.weight\": 262144,\n  \"module.backbone.0.body.layer3.2.conv1.weight\": 262144,\n  \"module.backbone.0.body.layer3.2.conv2.weight\": 589824,\n  \"module.backbone.0.body.layer3.2.conv3.weight\": 262144,\n  \"module.backbone.0.body.layer3.3.conv1.weight\": 262144,\n  \"module.backbone.0.body.layer3.3.conv2.weight\": 589824,\n  \"module.backbone.0.body.layer3.3.conv3.weight\": 262144,\n  \"module.backbone.0.body.layer3.4.conv1.weight\": 262144,\n  \"module.backbone.0.body.layer3.4.conv2.weight\": 589824,\n  \"module.backbone.0.body.layer3.4.conv3.weight\": 262144,\n  \"module.backbone.0.body.layer3.5.conv1.weight\": 262144,\n  \"module.backbone.0.body.layer3.5.conv2.weight\": 589824,\n  \"module.backbone.0.body.layer3.5.conv3.weight\": 262144,\n  \"module.backbone.0.body.layer4.0.conv1.weight\": 524288,\n  \"module.backbone.0.body.layer4.0.conv2.weight\": 2359296,\n  \"module.backbone.0.body.layer4.0.conv3.weight\": 1048576,\n  \"module.backbone.0.body.layer4.0.downsample.0.weight\": 2097152,\n  \"module.backbone.0.body.layer4.1.conv1.weight\": 1048576,\n  \"module.backbone.0.body.layer4.1.conv2.weight\": 2359296,\n  \"module.backbone.0.body.layer4.1.conv3.weight\": 1048576,\n  \"module.backbone.0.body.layer4.2.conv1.weight\": 1048576,\n  \"module.backbone.0.body.layer4.2.conv2.weight\": 2359296,\n  \"module.backbone.0.body.layer4.2.conv3.weight\": 1048576\n}\nstrong aug disabled\ndata_aug_params: {\n  \"scales\": [\n    480,\n    512,\n    544,\n    576,\n    608,\n    640,\n    672\n  ],\n  \"max_size\": 512,\n  \"scales2_resize\": [\n    400,\n    500,\n    600\n  ],\n  \"scales2_crop\": [\n    384,\n    600\n  ]\n}\nloading annotations into memory...\nDone (t=0.01s)\ncreating index...\nindex created!\nstrong aug disabled\ndata_aug_params: {\n  \"scales\": [\n    480,\n    512,\n    544,\n    576,\n    608,\n    640,\n    672\n  ],\n  \"max_size\": 512,\n  \"scales2_resize\": [\n    400,\n    500,\n    600\n  ],\n  \"scales2_crop\": [\n    384,\n    600\n  ]\n}\nValidation transform\nloading annotations into memory...\nDone (t=0.00s)\ncreating index...\nindex created!\nLength of training dataset: 4986\nLength of validation dataset: 588\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[12/09 12:40:15.294]: Ignore keys: []\n[12/09 12:40:15.366]: _IncompatibleKeys(missing_keys=['transformer.decoder.class_embed.0.weight', 'transformer.decoder.class_embed.0.bias', 'transformer.decoder.class_embed.1.weight', 'transformer.decoder.class_embed.1.bias', 'transformer.decoder.class_embed.2.weight', 'transformer.decoder.class_embed.2.bias', 'transformer.decoder.class_embed.3.weight', 'transformer.decoder.class_embed.3.bias', 'transformer.decoder.class_embed.4.weight', 'transformer.decoder.class_embed.4.bias', 'transformer.decoder.class_embed.5.weight', 'transformer.decoder.class_embed.5.bias', 'transformer.enc_out_class_embed.weight', 'transformer.enc_out_class_embed.bias', 'label_enc.weight', 'class_embed.0.weight', 'class_embed.0.bias', 'class_embed.1.weight', 'class_embed.1.bias', 'class_embed.2.weight', 'class_embed.2.bias', 'class_embed.3.weight', 'class_embed.3.bias', 'class_embed.4.weight', 'class_embed.4.bias', 'class_embed.5.weight', 'class_embed.5.bias'], unexpected_keys=[])\n/kaggle/working/DINO/engine.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\nStart training\n/kaggle/working/DINO/engine.py:24: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler(enabled=args.amp)\n/kaggle/working/DINO/engine.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=args.amp):\n/kaggle/working/DINO/engine.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=args.amp):\n/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n/kaggle/working/DINO/models/dino/dino.py:512: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n  t = torch.range(0, len(targets[i]['labels']) - 1).long().cuda()\n/kaggle/working/DINO/models/dino/dino.py:512: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n  t = torch.range(0, len(targets[i]['labels']) - 1).long().cuda()\nEpoch: [0]  [  0/311]  eta: 0:30:55  lr: 0.000100  class_error: 0.00  loss: 100.3997 (100.3997)  loss_bbox: 3.0210 (3.0210)  loss_bbox_0: 3.1680 (3.1680)  loss_bbox_1: 3.0063 (3.0063)  loss_bbox_2: 3.1619 (3.1619)  loss_bbox_3: 3.0881 (3.0881)  loss_bbox_4: 3.0176 (3.0176)  loss_bbox_dn: 5.8963 (5.8963)  loss_bbox_dn_0: 6.2049 (6.2049)  loss_bbox_dn_1: 4.7543 (4.7543)  loss_bbox_dn_2: 5.7475 (5.7475)  loss_bbox_dn_3: 5.7911 (5.7911)  loss_bbox_dn_4: 5.8329 (5.8329)  loss_bbox_interm: 3.1766 (3.1766)  loss_ce: 0.9523 (0.9523)  loss_ce_0: 1.1449 (1.1449)  loss_ce_1: 1.0213 (1.0213)  loss_ce_2: 1.0295 (1.0295)  loss_ce_3: 1.0266 (1.0266)  loss_ce_4: 0.9922 (0.9922)  loss_ce_dn: 1.1665 (1.1665)  loss_ce_dn_0: 1.4534 (1.4534)  loss_ce_dn_1: 1.2274 (1.2274)  loss_ce_dn_2: 1.2650 (1.2650)  loss_ce_dn_3: 1.2445 (1.2445)  loss_ce_dn_4: 1.1865 (1.1865)  loss_ce_interm: 1.1044 (1.1044)  loss_giou: 2.6281 (2.6281)  loss_giou_0: 2.6369 (2.6369)  loss_giou_1: 2.6564 (2.6564)  loss_giou_2: 2.6218 (2.6218)  loss_giou_3: 2.6327 (2.6327)  loss_giou_4: 2.6287 (2.6287)  loss_giou_dn: 1.9111 (1.9111)  loss_giou_dn_0: 1.9696 (1.9696)  loss_giou_dn_1: 1.9030 (1.9030)  loss_giou_dn_2: 1.9132 (1.9132)  loss_giou_dn_3: 1.9148 (1.9148)  loss_giou_dn_4: 1.9088 (1.9088)  loss_giou_interm: 2.3938 (2.3938)  cardinality_error_unscaled: 0.5625 (0.5625)  cardinality_error_0_unscaled: 0.5625 (0.5625)  cardinality_error_1_unscaled: 0.5625 (0.5625)  cardinality_error_2_unscaled: 0.5625 (0.5625)  cardinality_error_3_unscaled: 0.5625 (0.5625)  cardinality_error_4_unscaled: 0.5625 (0.5625)  cardinality_error_dn_unscaled: 0.5625 (0.5625)  cardinality_error_dn_0_unscaled: 0.5625 (0.5625)  cardinality_error_dn_1_unscaled: 0.5625 (0.5625)  cardinality_error_dn_2_unscaled: 0.5625 (0.5625)  cardinality_error_dn_3_unscaled: 0.5625 (0.5625)  cardinality_error_dn_4_unscaled: 0.5625 (0.5625)  cardinality_error_interm_unscaled: 0.5625 (0.5625)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.6042 (0.6042)  loss_bbox_0_unscaled: 0.6336 (0.6336)  loss_bbox_1_unscaled: 0.6013 (0.6013)  loss_bbox_2_unscaled: 0.6324 (0.6324)  loss_bbox_3_unscaled: 0.6176 (0.6176)  loss_bbox_4_unscaled: 0.6035 (0.6035)  loss_bbox_dn_unscaled: 1.1793 (1.1793)  loss_bbox_dn_0_unscaled: 1.2410 (1.2410)  loss_bbox_dn_1_unscaled: 0.9509 (0.9509)  loss_bbox_dn_2_unscaled: 1.1495 (1.1495)  loss_bbox_dn_3_unscaled: 1.1582 (1.1582)  loss_bbox_dn_4_unscaled: 1.1666 (1.1666)  loss_bbox_interm_unscaled: 0.6353 (0.6353)  loss_ce_unscaled: 0.9523 (0.9523)  loss_ce_0_unscaled: 1.1449 (1.1449)  loss_ce_1_unscaled: 1.0213 (1.0213)  loss_ce_2_unscaled: 1.0295 (1.0295)  loss_ce_3_unscaled: 1.0266 (1.0266)  loss_ce_4_unscaled: 0.9922 (0.9922)  loss_ce_dn_unscaled: 1.1665 (1.1665)  loss_ce_dn_0_unscaled: 1.4534 (1.4534)  loss_ce_dn_1_unscaled: 1.2274 (1.2274)  loss_ce_dn_2_unscaled: 1.2650 (1.2650)  loss_ce_dn_3_unscaled: 1.2445 (1.2445)  loss_ce_dn_4_unscaled: 1.1865 (1.1865)  loss_ce_interm_unscaled: 1.1044 (1.1044)  loss_giou_unscaled: 1.3140 (1.3140)  loss_giou_0_unscaled: 1.3184 (1.3184)  loss_giou_1_unscaled: 1.3282 (1.3282)  loss_giou_2_unscaled: 1.3109 (1.3109)  loss_giou_3_unscaled: 1.3164 (1.3164)  loss_giou_4_unscaled: 1.3143 (1.3143)  loss_giou_dn_unscaled: 0.9555 (0.9555)  loss_giou_dn_0_unscaled: 0.9848 (0.9848)  loss_giou_dn_1_unscaled: 0.9515 (0.9515)  loss_giou_dn_2_unscaled: 0.9566 (0.9566)  loss_giou_dn_3_unscaled: 0.9574 (0.9574)  loss_giou_dn_4_unscaled: 0.9544 (0.9544)  loss_giou_interm_unscaled: 1.1969 (1.1969)  loss_hw_unscaled: 0.2838 (0.2838)  loss_hw_0_unscaled: 0.3054 (0.3054)  loss_hw_1_unscaled: 0.2757 (0.2757)  loss_hw_2_unscaled: 0.3182 (0.3182)  loss_hw_3_unscaled: 0.2883 (0.2883)  loss_hw_4_unscaled: 0.2832 (0.2832)  loss_hw_dn_unscaled: 1.1341 (1.1341)  loss_hw_dn_0_unscaled: 1.2115 (1.2115)  loss_hw_dn_1_unscaled: 0.9133 (0.9133)  loss_hw_dn_2_unscaled: 1.1108 (1.1108)  loss_hw_dn_3_unscaled: 1.1178 (1.1178)  loss_hw_dn_4_unscaled: 1.1217 (1.1217)  loss_hw_interm_unscaled: 0.3349 (0.3349)  loss_xy_unscaled: 0.3204 (0.3204)  loss_xy_0_unscaled: 0.3282 (0.3282)  loss_xy_1_unscaled: 0.3256 (0.3256)  loss_xy_2_unscaled: 0.3141 (0.3141)  loss_xy_3_unscaled: 0.3293 (0.3293)  loss_xy_4_unscaled: 0.3203 (0.3203)  loss_xy_dn_unscaled: 0.0451 (0.0451)  loss_xy_dn_0_unscaled: 0.0295 (0.0295)  loss_xy_dn_1_unscaled: 0.0375 (0.0375)  loss_xy_dn_2_unscaled: 0.0387 (0.0387)  loss_xy_dn_3_unscaled: 0.0404 (0.0404)  loss_xy_dn_4_unscaled: 0.0449 (0.0449)  loss_xy_interm_unscaled: 0.3004 (0.3004)  time: 5.9665  data: 2.7866  max mem: 10500\nEpoch: [0]  [ 10/311]  eta: 0:10:19  lr: 0.000100  class_error: 0.00  loss: 33.9250 (42.2796)  loss_bbox: 0.8814 (1.2514)  loss_bbox_0: 0.9337 (1.3277)  loss_bbox_1: 0.9099 (1.2920)  loss_bbox_2: 0.9174 (1.2871)  loss_bbox_3: 0.8885 (1.2542)  loss_bbox_4: 0.8808 (1.2391)  loss_bbox_dn: 0.2615 (0.7874)  loss_bbox_dn_0: 0.3871 (1.0991)  loss_bbox_dn_1: 0.2476 (0.6910)  loss_bbox_dn_2: 0.2601 (0.7771)  loss_bbox_dn_3: 0.2585 (0.7803)  loss_bbox_dn_4: 0.2614 (0.7812)  loss_bbox_interm: 0.9986 (1.2775)  loss_ce: 0.7920 (0.7953)  loss_ce_0: 0.8960 (0.8993)  loss_ce_1: 0.9047 (0.8776)  loss_ce_2: 0.8503 (0.8362)  loss_ce_3: 0.8113 (0.8307)  loss_ce_4: 0.7330 (0.7889)  loss_ce_dn: 0.5475 (0.6295)  loss_ce_dn_0: 0.9390 (0.9979)  loss_ce_dn_1: 0.7964 (0.7972)  loss_ce_dn_2: 0.6654 (0.7136)  loss_ce_dn_3: 0.6802 (0.7302)  loss_ce_dn_4: 0.5649 (0.6411)  loss_ce_interm: 1.0662 (1.0616)  loss_giou: 1.3876 (1.5908)  loss_giou_0: 1.5496 (1.6615)  loss_giou_1: 1.5305 (1.6344)  loss_giou_2: 1.5138 (1.6285)  loss_giou_3: 1.4949 (1.6131)  loss_giou_4: 1.3794 (1.6007)  loss_giou_dn: 0.9324 (1.0505)  loss_giou_dn_0: 0.9958 (1.1837)  loss_giou_dn_1: 0.9236 (1.0419)  loss_giou_dn_2: 0.9346 (1.0484)  loss_giou_dn_3: 0.9258 (1.0459)  loss_giou_dn_4: 0.9322 (1.0508)  loss_giou_interm: 1.5358 (1.6848)  cardinality_error_unscaled: 0.4375 (0.4489)  cardinality_error_0_unscaled: 0.4375 (0.4489)  cardinality_error_1_unscaled: 0.4375 (0.4489)  cardinality_error_2_unscaled: 0.4375 (0.4489)  cardinality_error_3_unscaled: 0.4375 (0.4489)  cardinality_error_4_unscaled: 0.4375 (0.4489)  cardinality_error_dn_unscaled: 0.4375 (0.4489)  cardinality_error_dn_0_unscaled: 0.4375 (0.4489)  cardinality_error_dn_1_unscaled: 0.4375 (0.4489)  cardinality_error_dn_2_unscaled: 0.4375 (0.4489)  cardinality_error_dn_3_unscaled: 0.4375 (0.4489)  cardinality_error_dn_4_unscaled: 0.4375 (0.4489)  cardinality_error_interm_unscaled: 0.4375 (0.4489)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.1763 (0.2503)  loss_bbox_0_unscaled: 0.1867 (0.2655)  loss_bbox_1_unscaled: 0.1820 (0.2584)  loss_bbox_2_unscaled: 0.1835 (0.2574)  loss_bbox_3_unscaled: 0.1777 (0.2508)  loss_bbox_4_unscaled: 0.1762 (0.2478)  loss_bbox_dn_unscaled: 0.0523 (0.1575)  loss_bbox_dn_0_unscaled: 0.0774 (0.2198)  loss_bbox_dn_1_unscaled: 0.0495 (0.1382)  loss_bbox_dn_2_unscaled: 0.0520 (0.1554)  loss_bbox_dn_3_unscaled: 0.0517 (0.1561)  loss_bbox_dn_4_unscaled: 0.0523 (0.1562)  loss_bbox_interm_unscaled: 0.1997 (0.2555)  loss_ce_unscaled: 0.7920 (0.7953)  loss_ce_0_unscaled: 0.8960 (0.8993)  loss_ce_1_unscaled: 0.9047 (0.8776)  loss_ce_2_unscaled: 0.8503 (0.8362)  loss_ce_3_unscaled: 0.8113 (0.8307)  loss_ce_4_unscaled: 0.7330 (0.7889)  loss_ce_dn_unscaled: 0.5475 (0.6295)  loss_ce_dn_0_unscaled: 0.9390 (0.9979)  loss_ce_dn_1_unscaled: 0.7964 (0.7972)  loss_ce_dn_2_unscaled: 0.6654 (0.7136)  loss_ce_dn_3_unscaled: 0.6802 (0.7302)  loss_ce_dn_4_unscaled: 0.5649 (0.6411)  loss_ce_interm_unscaled: 1.0662 (1.0616)  loss_giou_unscaled: 0.6938 (0.7954)  loss_giou_0_unscaled: 0.7748 (0.8307)  loss_giou_1_unscaled: 0.7653 (0.8172)  loss_giou_2_unscaled: 0.7569 (0.8142)  loss_giou_3_unscaled: 0.7474 (0.8066)  loss_giou_4_unscaled: 0.6897 (0.8004)  loss_giou_dn_unscaled: 0.4662 (0.5253)  loss_giou_dn_0_unscaled: 0.4979 (0.5918)  loss_giou_dn_1_unscaled: 0.4618 (0.5209)  loss_giou_dn_2_unscaled: 0.4673 (0.5242)  loss_giou_dn_3_unscaled: 0.4629 (0.5230)  loss_giou_dn_4_unscaled: 0.4661 (0.5254)  loss_giou_interm_unscaled: 0.7679 (0.8424)  loss_hw_unscaled: 0.1079 (0.1311)  loss_hw_0_unscaled: 0.1173 (0.1306)  loss_hw_1_unscaled: 0.1229 (0.1325)  loss_hw_2_unscaled: 0.1054 (0.1315)  loss_hw_3_unscaled: 0.0953 (0.1240)  loss_hw_4_unscaled: 0.1077 (0.1246)  loss_hw_dn_unscaled: 0.0319 (0.1342)  loss_hw_dn_0_unscaled: 0.0525 (0.1944)  loss_hw_dn_1_unscaled: 0.0306 (0.1159)  loss_hw_dn_2_unscaled: 0.0336 (0.1327)  loss_hw_dn_3_unscaled: 0.0335 (0.1333)  loss_hw_dn_4_unscaled: 0.0317 (0.1330)  loss_hw_interm_unscaled: 0.1358 (0.1217)  loss_xy_unscaled: 0.0903 (0.1191)  loss_xy_0_unscaled: 0.0900 (0.1350)  loss_xy_1_unscaled: 0.0924 (0.1259)  loss_xy_2_unscaled: 0.0941 (0.1259)  loss_xy_3_unscaled: 0.1073 (0.1268)  loss_xy_4_unscaled: 0.0904 (0.1232)  loss_xy_dn_unscaled: 0.0210 (0.0233)  loss_xy_dn_0_unscaled: 0.0249 (0.0254)  loss_xy_dn_1_unscaled: 0.0197 (0.0223)  loss_xy_dn_2_unscaled: 0.0207 (0.0227)  loss_xy_dn_3_unscaled: 0.0206 (0.0227)  loss_xy_dn_4_unscaled: 0.0210 (0.0233)  loss_xy_interm_unscaled: 0.0900 (0.1339)  time: 2.0569  data: 0.2750  max mem: 10855\nEpoch: [0]  [ 20/311]  eta: 0:09:05  lr: 0.000100  class_error: 0.00  loss: 24.7233 (31.3875)  loss_bbox: 0.3541 (0.8038)  loss_bbox_0: 0.4540 (0.8701)  loss_bbox_1: 0.4238 (0.8342)  loss_bbox_2: 0.4251 (0.8219)  loss_bbox_3: 0.4157 (0.8038)  loss_bbox_4: 0.3558 (0.7950)  loss_bbox_dn: 0.1866 (0.4919)  loss_bbox_dn_0: 0.2478 (0.6791)  loss_bbox_dn_1: 0.1923 (0.4458)  loss_bbox_dn_2: 0.1899 (0.4862)  loss_bbox_dn_3: 0.1848 (0.4898)  loss_bbox_dn_4: 0.1850 (0.4881)  loss_bbox_interm: 0.4737 (0.8536)  loss_ce: 0.5948 (0.6115)  loss_ce_0: 0.6924 (0.7606)  loss_ce_1: 0.7389 (0.7607)  loss_ce_2: 0.6335 (0.6938)  loss_ce_3: 0.6554 (0.6874)  loss_ce_4: 0.6276 (0.6323)  loss_ce_dn: 0.2506 (0.3829)  loss_ce_dn_0: 0.6241 (0.7596)  loss_ce_dn_1: 0.4172 (0.5259)  loss_ce_dn_2: 0.2723 (0.4334)  loss_ce_dn_3: 0.2081 (0.4291)  loss_ce_dn_4: 0.2270 (0.3925)  loss_ce_interm: 1.0109 (1.0216)  loss_giou: 1.0983 (1.2682)  loss_giou_0: 1.2301 (1.3579)  loss_giou_1: 1.1741 (1.3074)  loss_giou_2: 1.0999 (1.2932)  loss_giou_3: 1.0870 (1.2855)  loss_giou_4: 1.0975 (1.2660)  loss_giou_dn: 0.8167 (0.8562)  loss_giou_dn_0: 0.8686 (0.9924)  loss_giou_dn_1: 0.7981 (0.8594)  loss_giou_dn_2: 0.8017 (0.8506)  loss_giou_dn_3: 0.8120 (0.8537)  loss_giou_dn_4: 0.8063 (0.8539)  loss_giou_interm: 1.2975 (1.3881)  cardinality_error_unscaled: 0.5000 (0.4911)  cardinality_error_0_unscaled: 0.5000 (0.4911)  cardinality_error_1_unscaled: 0.5000 (0.4911)  cardinality_error_2_unscaled: 0.5000 (0.4911)  cardinality_error_3_unscaled: 0.5000 (0.4911)  cardinality_error_4_unscaled: 0.5000 (0.4911)  cardinality_error_dn_unscaled: 0.5000 (0.4911)  cardinality_error_dn_0_unscaled: 0.5000 (0.4911)  cardinality_error_dn_1_unscaled: 0.5000 (0.4911)  cardinality_error_dn_2_unscaled: 0.5000 (0.4911)  cardinality_error_dn_3_unscaled: 0.5000 (0.4911)  cardinality_error_dn_4_unscaled: 0.5000 (0.4911)  cardinality_error_interm_unscaled: 0.5000 (0.4911)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0708 (0.1608)  loss_bbox_0_unscaled: 0.0908 (0.1740)  loss_bbox_1_unscaled: 0.0848 (0.1668)  loss_bbox_2_unscaled: 0.0850 (0.1644)  loss_bbox_3_unscaled: 0.0831 (0.1608)  loss_bbox_4_unscaled: 0.0712 (0.1590)  loss_bbox_dn_unscaled: 0.0373 (0.0984)  loss_bbox_dn_0_unscaled: 0.0496 (0.1358)  loss_bbox_dn_1_unscaled: 0.0385 (0.0892)  loss_bbox_dn_2_unscaled: 0.0380 (0.0972)  loss_bbox_dn_3_unscaled: 0.0370 (0.0980)  loss_bbox_dn_4_unscaled: 0.0370 (0.0976)  loss_bbox_interm_unscaled: 0.0947 (0.1707)  loss_ce_unscaled: 0.5948 (0.6115)  loss_ce_0_unscaled: 0.6924 (0.7606)  loss_ce_1_unscaled: 0.7389 (0.7607)  loss_ce_2_unscaled: 0.6335 (0.6938)  loss_ce_3_unscaled: 0.6554 (0.6874)  loss_ce_4_unscaled: 0.6276 (0.6323)  loss_ce_dn_unscaled: 0.2506 (0.3829)  loss_ce_dn_0_unscaled: 0.6241 (0.7596)  loss_ce_dn_1_unscaled: 0.4172 (0.5259)  loss_ce_dn_2_unscaled: 0.2723 (0.4334)  loss_ce_dn_3_unscaled: 0.2081 (0.4291)  loss_ce_dn_4_unscaled: 0.2270 (0.3925)  loss_ce_interm_unscaled: 1.0109 (1.0216)  loss_giou_unscaled: 0.5491 (0.6341)  loss_giou_0_unscaled: 0.6151 (0.6790)  loss_giou_1_unscaled: 0.5871 (0.6537)  loss_giou_2_unscaled: 0.5499 (0.6466)  loss_giou_3_unscaled: 0.5435 (0.6427)  loss_giou_4_unscaled: 0.5487 (0.6330)  loss_giou_dn_unscaled: 0.4084 (0.4281)  loss_giou_dn_0_unscaled: 0.4343 (0.4962)  loss_giou_dn_1_unscaled: 0.3991 (0.4297)  loss_giou_dn_2_unscaled: 0.4008 (0.4253)  loss_giou_dn_3_unscaled: 0.4060 (0.4269)  loss_giou_dn_4_unscaled: 0.4032 (0.4270)  loss_giou_interm_unscaled: 0.6487 (0.6941)  loss_hw_unscaled: 0.0456 (0.0873)  loss_hw_0_unscaled: 0.0525 (0.0880)  loss_hw_1_unscaled: 0.0468 (0.0868)  loss_hw_2_unscaled: 0.0457 (0.0854)  loss_hw_3_unscaled: 0.0459 (0.0813)  loss_hw_4_unscaled: 0.0454 (0.0837)  loss_hw_dn_unscaled: 0.0233 (0.0805)  loss_hw_dn_0_unscaled: 0.0327 (0.1160)  loss_hw_dn_1_unscaled: 0.0252 (0.0718)  loss_hw_dn_2_unscaled: 0.0240 (0.0799)  loss_hw_dn_3_unscaled: 0.0237 (0.0806)  loss_hw_dn_4_unscaled: 0.0233 (0.0799)  loss_hw_interm_unscaled: 0.0508 (0.0845)  loss_xy_unscaled: 0.0287 (0.0735)  loss_xy_0_unscaled: 0.0407 (0.0860)  loss_xy_1_unscaled: 0.0353 (0.0800)  loss_xy_2_unscaled: 0.0319 (0.0790)  loss_xy_3_unscaled: 0.0310 (0.0794)  loss_xy_4_unscaled: 0.0291 (0.0753)  loss_xy_dn_unscaled: 0.0147 (0.0178)  loss_xy_dn_0_unscaled: 0.0171 (0.0199)  loss_xy_dn_1_unscaled: 0.0143 (0.0173)  loss_xy_dn_2_unscaled: 0.0141 (0.0174)  loss_xy_dn_3_unscaled: 0.0148 (0.0174)  loss_xy_dn_4_unscaled: 0.0147 (0.0177)  loss_xy_interm_unscaled: 0.0456 (0.0862)  time: 1.6705  data: 0.0197  max mem: 10856\nEpoch: [0]  [ 30/311]  eta: 0:08:32  lr: 0.000100  class_error: 0.00  loss: 14.4597 (25.5087)  loss_bbox: 0.1751 (0.5928)  loss_bbox_0: 0.1343 (0.6312)  loss_bbox_1: 0.1483 (0.6066)  loss_bbox_2: 0.1402 (0.6041)  loss_bbox_3: 0.1561 (0.5928)  loss_bbox_4: 0.1670 (0.5857)  loss_bbox_dn: 0.1390 (0.3707)  loss_bbox_dn_0: 0.1649 (0.5062)  loss_bbox_dn_1: 0.1379 (0.3396)  loss_bbox_dn_2: 0.1332 (0.3655)  loss_bbox_dn_3: 0.1372 (0.3683)  loss_bbox_dn_4: 0.1348 (0.3679)  loss_bbox_interm: 0.1519 (0.6288)  loss_ce: 0.3085 (0.4946)  loss_ce_0: 0.5333 (0.6659)  loss_ce_1: 0.5138 (0.6601)  loss_ce_2: 0.4487 (0.5891)  loss_ce_3: 0.3708 (0.5696)  loss_ce_4: 0.3231 (0.5154)  loss_ce_dn: 0.0544 (0.2700)  loss_ce_dn_0: 0.3905 (0.6202)  loss_ce_dn_1: 0.1131 (0.3827)  loss_ce_dn_2: 0.0705 (0.3110)  loss_ce_dn_3: 0.0633 (0.3064)  loss_ce_dn_4: 0.0644 (0.2798)  loss_ce_interm: 0.9446 (0.9841)  loss_giou: 0.7493 (1.0761)  loss_giou_0: 0.6341 (1.1135)  loss_giou_1: 0.6443 (1.0755)  loss_giou_2: 0.7106 (1.0748)  loss_giou_3: 0.7101 (1.0707)  loss_giou_4: 0.7566 (1.0712)  loss_giou_dn: 0.5875 (0.7597)  loss_giou_dn_0: 0.7148 (0.8789)  loss_giou_dn_1: 0.5993 (0.7641)  loss_giou_dn_2: 0.5820 (0.7518)  loss_giou_dn_3: 0.5738 (0.7546)  loss_giou_dn_4: 0.5821 (0.7570)  loss_giou_interm: 0.6923 (1.1515)  cardinality_error_unscaled: 0.5625 (0.5000)  cardinality_error_0_unscaled: 0.5625 (0.5000)  cardinality_error_1_unscaled: 0.5625 (0.5000)  cardinality_error_2_unscaled: 0.5625 (0.5000)  cardinality_error_3_unscaled: 0.5625 (0.5000)  cardinality_error_4_unscaled: 0.5625 (0.5000)  cardinality_error_dn_unscaled: 0.5625 (0.5000)  cardinality_error_dn_0_unscaled: 0.5625 (0.5000)  cardinality_error_dn_1_unscaled: 0.5625 (0.5000)  cardinality_error_dn_2_unscaled: 0.5625 (0.5000)  cardinality_error_dn_3_unscaled: 0.5625 (0.5000)  cardinality_error_dn_4_unscaled: 0.5625 (0.5000)  cardinality_error_interm_unscaled: 0.5625 (0.5000)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0350 (0.1186)  loss_bbox_0_unscaled: 0.0269 (0.1262)  loss_bbox_1_unscaled: 0.0297 (0.1213)  loss_bbox_2_unscaled: 0.0280 (0.1208)  loss_bbox_3_unscaled: 0.0312 (0.1186)  loss_bbox_4_unscaled: 0.0334 (0.1171)  loss_bbox_dn_unscaled: 0.0278 (0.0741)  loss_bbox_dn_0_unscaled: 0.0330 (0.1012)  loss_bbox_dn_1_unscaled: 0.0276 (0.0679)  loss_bbox_dn_2_unscaled: 0.0266 (0.0731)  loss_bbox_dn_3_unscaled: 0.0274 (0.0737)  loss_bbox_dn_4_unscaled: 0.0270 (0.0736)  loss_bbox_interm_unscaled: 0.0304 (0.1258)  loss_ce_unscaled: 0.3085 (0.4946)  loss_ce_0_unscaled: 0.5333 (0.6659)  loss_ce_1_unscaled: 0.5138 (0.6601)  loss_ce_2_unscaled: 0.4487 (0.5891)  loss_ce_3_unscaled: 0.3708 (0.5696)  loss_ce_4_unscaled: 0.3231 (0.5154)  loss_ce_dn_unscaled: 0.0544 (0.2700)  loss_ce_dn_0_unscaled: 0.3905 (0.6202)  loss_ce_dn_1_unscaled: 0.1131 (0.3827)  loss_ce_dn_2_unscaled: 0.0705 (0.3110)  loss_ce_dn_3_unscaled: 0.0633 (0.3064)  loss_ce_dn_4_unscaled: 0.0644 (0.2798)  loss_ce_interm_unscaled: 0.9446 (0.9841)  loss_giou_unscaled: 0.3747 (0.5381)  loss_giou_0_unscaled: 0.3170 (0.5568)  loss_giou_1_unscaled: 0.3221 (0.5378)  loss_giou_2_unscaled: 0.3553 (0.5374)  loss_giou_3_unscaled: 0.3550 (0.5354)  loss_giou_4_unscaled: 0.3783 (0.5356)  loss_giou_dn_unscaled: 0.2937 (0.3799)  loss_giou_dn_0_unscaled: 0.3574 (0.4394)  loss_giou_dn_1_unscaled: 0.2997 (0.3820)  loss_giou_dn_2_unscaled: 0.2910 (0.3759)  loss_giou_dn_3_unscaled: 0.2869 (0.3773)  loss_giou_dn_4_unscaled: 0.2911 (0.3785)  loss_giou_interm_unscaled: 0.3462 (0.5758)  loss_hw_unscaled: 0.0226 (0.0658)  loss_hw_0_unscaled: 0.0174 (0.0649)  loss_hw_1_unscaled: 0.0182 (0.0645)  loss_hw_2_unscaled: 0.0192 (0.0643)  loss_hw_3_unscaled: 0.0203 (0.0621)  loss_hw_4_unscaled: 0.0219 (0.0632)  loss_hw_dn_unscaled: 0.0171 (0.0595)  loss_hw_dn_0_unscaled: 0.0223 (0.0849)  loss_hw_dn_1_unscaled: 0.0178 (0.0537)  loss_hw_dn_2_unscaled: 0.0164 (0.0589)  loss_hw_dn_3_unscaled: 0.0170 (0.0595)  loss_hw_dn_4_unscaled: 0.0168 (0.0590)  loss_hw_interm_unscaled: 0.0213 (0.0639)  loss_xy_unscaled: 0.0113 (0.0528)  loss_xy_0_unscaled: 0.0113 (0.0613)  loss_xy_1_unscaled: 0.0090 (0.0568)  loss_xy_2_unscaled: 0.0094 (0.0565)  loss_xy_3_unscaled: 0.0088 (0.0565)  loss_xy_4_unscaled: 0.0115 (0.0540)  loss_xy_dn_unscaled: 0.0098 (0.0146)  loss_xy_dn_0_unscaled: 0.0110 (0.0163)  loss_xy_dn_1_unscaled: 0.0093 (0.0143)  loss_xy_dn_2_unscaled: 0.0092 (0.0142)  loss_xy_dn_3_unscaled: 0.0094 (0.0142)  loss_xy_dn_4_unscaled: 0.0093 (0.0145)  loss_xy_interm_unscaled: 0.0109 (0.0618)  time: 1.6935  data: 0.0215  max mem: 10857\nEpoch: [0]  [ 40/311]  eta: 0:08:09  lr: 0.000100  class_error: 0.00  loss: 11.6212 (21.7624)  loss_bbox: 0.1351 (0.4804)  loss_bbox_0: 0.0971 (0.5009)  loss_bbox_1: 0.1057 (0.4847)  loss_bbox_2: 0.1169 (0.4847)  loss_bbox_3: 0.1322 (0.4804)  loss_bbox_4: 0.1315 (0.4742)  loss_bbox_dn: 0.1059 (0.3057)  loss_bbox_dn_0: 0.1401 (0.4181)  loss_bbox_dn_1: 0.1078 (0.2839)  loss_bbox_dn_2: 0.1032 (0.3021)  loss_bbox_dn_3: 0.1035 (0.3041)  loss_bbox_dn_4: 0.1050 (0.3035)  loss_bbox_interm: 0.1245 (0.5024)  loss_ce: 0.1721 (0.4073)  loss_ce_0: 0.4264 (0.6082)  loss_ce_1: 0.3769 (0.5785)  loss_ce_2: 0.3116 (0.5041)  loss_ce_3: 0.2649 (0.4729)  loss_ce_4: 0.1669 (0.4259)  loss_ce_dn: 0.0203 (0.2072)  loss_ce_dn_0: 0.2510 (0.5182)  loss_ce_dn_1: 0.0575 (0.3004)  loss_ce_dn_2: 0.0323 (0.2409)  loss_ce_dn_3: 0.0253 (0.2364)  loss_ce_dn_4: 0.0261 (0.2159)  loss_ce_interm: 0.8343 (0.9434)  loss_giou: 0.5829 (0.9395)  loss_giou_0: 0.5076 (0.9418)  loss_giou_1: 0.4811 (0.9190)  loss_giou_2: 0.5175 (0.9265)  loss_giou_3: 0.5599 (0.9319)  loss_giou_4: 0.5851 (0.9353)  loss_giou_dn: 0.4863 (0.6806)  loss_giou_dn_0: 0.6039 (0.7977)  loss_giou_dn_1: 0.4971 (0.6887)  loss_giou_dn_2: 0.4774 (0.6758)  loss_giou_dn_3: 0.4879 (0.6773)  loss_giou_dn_4: 0.4875 (0.6782)  loss_giou_interm: 0.5132 (0.9856)  cardinality_error_unscaled: 0.5000 (0.4863)  cardinality_error_0_unscaled: 0.5000 (0.4863)  cardinality_error_1_unscaled: 0.5000 (0.4863)  cardinality_error_2_unscaled: 0.5000 (0.4863)  cardinality_error_3_unscaled: 0.5000 (0.4863)  cardinality_error_4_unscaled: 0.5000 (0.4863)  cardinality_error_dn_unscaled: 0.5000 (0.4863)  cardinality_error_dn_0_unscaled: 0.5000 (0.4863)  cardinality_error_dn_1_unscaled: 0.5000 (0.4863)  cardinality_error_dn_2_unscaled: 0.5000 (0.4863)  cardinality_error_dn_3_unscaled: 0.5000 (0.4863)  cardinality_error_dn_4_unscaled: 0.5000 (0.4863)  cardinality_error_interm_unscaled: 0.5000 (0.4863)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0270 (0.0961)  loss_bbox_0_unscaled: 0.0194 (0.1002)  loss_bbox_1_unscaled: 0.0211 (0.0969)  loss_bbox_2_unscaled: 0.0234 (0.0969)  loss_bbox_3_unscaled: 0.0264 (0.0961)  loss_bbox_4_unscaled: 0.0263 (0.0948)  loss_bbox_dn_unscaled: 0.0212 (0.0611)  loss_bbox_dn_0_unscaled: 0.0280 (0.0836)  loss_bbox_dn_1_unscaled: 0.0216 (0.0568)  loss_bbox_dn_2_unscaled: 0.0206 (0.0604)  loss_bbox_dn_3_unscaled: 0.0207 (0.0608)  loss_bbox_dn_4_unscaled: 0.0210 (0.0607)  loss_bbox_interm_unscaled: 0.0249 (0.1005)  loss_ce_unscaled: 0.1721 (0.4073)  loss_ce_0_unscaled: 0.4264 (0.6082)  loss_ce_1_unscaled: 0.3769 (0.5785)  loss_ce_2_unscaled: 0.3116 (0.5041)  loss_ce_3_unscaled: 0.2649 (0.4729)  loss_ce_4_unscaled: 0.1669 (0.4259)  loss_ce_dn_unscaled: 0.0203 (0.2072)  loss_ce_dn_0_unscaled: 0.2510 (0.5182)  loss_ce_dn_1_unscaled: 0.0575 (0.3004)  loss_ce_dn_2_unscaled: 0.0323 (0.2409)  loss_ce_dn_3_unscaled: 0.0253 (0.2364)  loss_ce_dn_4_unscaled: 0.0261 (0.2159)  loss_ce_interm_unscaled: 0.8343 (0.9434)  loss_giou_unscaled: 0.2915 (0.4697)  loss_giou_0_unscaled: 0.2538 (0.4709)  loss_giou_1_unscaled: 0.2406 (0.4595)  loss_giou_2_unscaled: 0.2588 (0.4633)  loss_giou_3_unscaled: 0.2799 (0.4660)  loss_giou_4_unscaled: 0.2926 (0.4677)  loss_giou_dn_unscaled: 0.2432 (0.3403)  loss_giou_dn_0_unscaled: 0.3019 (0.3988)  loss_giou_dn_1_unscaled: 0.2486 (0.3443)  loss_giou_dn_2_unscaled: 0.2387 (0.3379)  loss_giou_dn_3_unscaled: 0.2439 (0.3386)  loss_giou_dn_4_unscaled: 0.2438 (0.3391)  loss_giou_interm_unscaled: 0.2566 (0.4928)  loss_hw_unscaled: 0.0181 (0.0539)  loss_hw_0_unscaled: 0.0125 (0.0521)  loss_hw_1_unscaled: 0.0145 (0.0521)  loss_hw_2_unscaled: 0.0158 (0.0522)  loss_hw_3_unscaled: 0.0179 (0.0511)  loss_hw_4_unscaled: 0.0180 (0.0517)  loss_hw_dn_unscaled: 0.0136 (0.0481)  loss_hw_dn_0_unscaled: 0.0188 (0.0688)  loss_hw_dn_1_unscaled: 0.0147 (0.0440)  loss_hw_dn_2_unscaled: 0.0139 (0.0477)  loss_hw_dn_3_unscaled: 0.0138 (0.0482)  loss_hw_dn_4_unscaled: 0.0136 (0.0478)  loss_hw_interm_unscaled: 0.0150 (0.0516)  loss_xy_unscaled: 0.0082 (0.0422)  loss_xy_0_unscaled: 0.0065 (0.0481)  loss_xy_1_unscaled: 0.0070 (0.0449)  loss_xy_2_unscaled: 0.0079 (0.0448)  loss_xy_3_unscaled: 0.0077 (0.0450)  loss_xy_4_unscaled: 0.0080 (0.0431)  loss_xy_dn_unscaled: 0.0074 (0.0130)  loss_xy_dn_0_unscaled: 0.0090 (0.0148)  loss_xy_dn_1_unscaled: 0.0075 (0.0128)  loss_xy_dn_2_unscaled: 0.0072 (0.0127)  loss_xy_dn_3_unscaled: 0.0070 (0.0127)  loss_xy_dn_4_unscaled: 0.0072 (0.0129)  loss_xy_interm_unscaled: 0.0079 (0.0489)  time: 1.7330  data: 0.0261  max mem: 10857\nEpoch: [0]  [ 50/311]  eta: 0:07:49  lr: 0.000100  class_error: 0.00  loss: 9.6796 (19.4390)  loss_bbox: 0.1126 (0.4079)  loss_bbox_0: 0.0875 (0.4190)  loss_bbox_1: 0.0966 (0.4087)  loss_bbox_2: 0.1105 (0.4106)  loss_bbox_3: 0.1171 (0.4073)  loss_bbox_4: 0.1122 (0.4018)  loss_bbox_dn: 0.0941 (0.2622)  loss_bbox_dn_0: 0.1401 (0.3614)  loss_bbox_dn_1: 0.0998 (0.2460)  loss_bbox_dn_2: 0.0946 (0.2593)  loss_bbox_dn_3: 0.0947 (0.2610)  loss_bbox_dn_4: 0.0942 (0.2603)  loss_bbox_interm: 0.0968 (0.4216)  loss_ce: 0.1216 (0.3535)  loss_ce_0: 0.4127 (0.5697)  loss_ce_1: 0.2908 (0.5170)  loss_ce_2: 0.2053 (0.4404)  loss_ce_3: 0.1488 (0.4102)  loss_ce_4: 0.1390 (0.3687)  loss_ce_dn: 0.0074 (0.1681)  loss_ce_dn_0: 0.1549 (0.4402)  loss_ce_dn_1: 0.0360 (0.2486)  loss_ce_dn_2: 0.0176 (0.1967)  loss_ce_dn_3: 0.0106 (0.1925)  loss_ce_dn_4: 0.0136 (0.1761)  loss_ce_interm: 0.7812 (0.8990)  loss_giou: 0.5549 (0.8666)  loss_giou_0: 0.4346 (0.8444)  loss_giou_1: 0.4676 (0.8339)  loss_giou_2: 0.5158 (0.8522)  loss_giou_3: 0.5247 (0.8562)  loss_giou_4: 0.5186 (0.8603)  loss_giou_dn: 0.4450 (0.6350)  loss_giou_dn_0: 0.5543 (0.7573)  loss_giou_dn_1: 0.4514 (0.6454)  loss_giou_dn_2: 0.4400 (0.6308)  loss_giou_dn_3: 0.4314 (0.6322)  loss_giou_dn_4: 0.4365 (0.6326)  loss_giou_interm: 0.4465 (0.8847)  cardinality_error_unscaled: 0.4375 (0.4963)  cardinality_error_0_unscaled: 0.4375 (0.4963)  cardinality_error_1_unscaled: 0.4375 (0.4963)  cardinality_error_2_unscaled: 0.4375 (0.4963)  cardinality_error_3_unscaled: 0.4375 (0.4963)  cardinality_error_4_unscaled: 0.4375 (0.4963)  cardinality_error_dn_unscaled: 0.4375 (0.4963)  cardinality_error_dn_0_unscaled: 0.4375 (0.4963)  cardinality_error_dn_1_unscaled: 0.4375 (0.4963)  cardinality_error_dn_2_unscaled: 0.4375 (0.4963)  cardinality_error_dn_3_unscaled: 0.4375 (0.4963)  cardinality_error_dn_4_unscaled: 0.4375 (0.4963)  cardinality_error_interm_unscaled: 0.4375 (0.4963)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0225 (0.0816)  loss_bbox_0_unscaled: 0.0175 (0.0838)  loss_bbox_1_unscaled: 0.0193 (0.0817)  loss_bbox_2_unscaled: 0.0221 (0.0821)  loss_bbox_3_unscaled: 0.0234 (0.0815)  loss_bbox_4_unscaled: 0.0224 (0.0804)  loss_bbox_dn_unscaled: 0.0188 (0.0524)  loss_bbox_dn_0_unscaled: 0.0280 (0.0723)  loss_bbox_dn_1_unscaled: 0.0200 (0.0492)  loss_bbox_dn_2_unscaled: 0.0189 (0.0519)  loss_bbox_dn_3_unscaled: 0.0189 (0.0522)  loss_bbox_dn_4_unscaled: 0.0188 (0.0521)  loss_bbox_interm_unscaled: 0.0194 (0.0843)  loss_ce_unscaled: 0.1216 (0.3535)  loss_ce_0_unscaled: 0.4127 (0.5697)  loss_ce_1_unscaled: 0.2908 (0.5170)  loss_ce_2_unscaled: 0.2053 (0.4404)  loss_ce_3_unscaled: 0.1488 (0.4102)  loss_ce_4_unscaled: 0.1390 (0.3687)  loss_ce_dn_unscaled: 0.0074 (0.1681)  loss_ce_dn_0_unscaled: 0.1549 (0.4402)  loss_ce_dn_1_unscaled: 0.0360 (0.2486)  loss_ce_dn_2_unscaled: 0.0176 (0.1967)  loss_ce_dn_3_unscaled: 0.0106 (0.1925)  loss_ce_dn_4_unscaled: 0.0136 (0.1761)  loss_ce_interm_unscaled: 0.7812 (0.8990)  loss_giou_unscaled: 0.2774 (0.4333)  loss_giou_0_unscaled: 0.2173 (0.4222)  loss_giou_1_unscaled: 0.2338 (0.4169)  loss_giou_2_unscaled: 0.2579 (0.4261)  loss_giou_3_unscaled: 0.2624 (0.4281)  loss_giou_4_unscaled: 0.2593 (0.4301)  loss_giou_dn_unscaled: 0.2225 (0.3175)  loss_giou_dn_0_unscaled: 0.2772 (0.3786)  loss_giou_dn_1_unscaled: 0.2257 (0.3227)  loss_giou_dn_2_unscaled: 0.2200 (0.3154)  loss_giou_dn_3_unscaled: 0.2157 (0.3161)  loss_giou_dn_4_unscaled: 0.2183 (0.3163)  loss_giou_interm_unscaled: 0.2232 (0.4424)  loss_hw_unscaled: 0.0157 (0.0463)  loss_hw_0_unscaled: 0.0113 (0.0440)  loss_hw_1_unscaled: 0.0131 (0.0445)  loss_hw_2_unscaled: 0.0154 (0.0449)  loss_hw_3_unscaled: 0.0164 (0.0440)  loss_hw_4_unscaled: 0.0154 (0.0444)  loss_hw_dn_unscaled: 0.0125 (0.0409)  loss_hw_dn_0_unscaled: 0.0188 (0.0587)  loss_hw_dn_1_unscaled: 0.0125 (0.0377)  loss_hw_dn_2_unscaled: 0.0122 (0.0406)  loss_hw_dn_3_unscaled: 0.0129 (0.0409)  loss_hw_dn_4_unscaled: 0.0126 (0.0406)  loss_hw_interm_unscaled: 0.0119 (0.0438)  loss_xy_unscaled: 0.0072 (0.0353)  loss_xy_0_unscaled: 0.0056 (0.0398)  loss_xy_1_unscaled: 0.0062 (0.0372)  loss_xy_2_unscaled: 0.0070 (0.0373)  loss_xy_3_unscaled: 0.0071 (0.0375)  loss_xy_4_unscaled: 0.0071 (0.0360)  loss_xy_dn_unscaled: 0.0065 (0.0116)  loss_xy_dn_0_unscaled: 0.0090 (0.0135)  loss_xy_dn_1_unscaled: 0.0069 (0.0115)  loss_xy_dn_2_unscaled: 0.0068 (0.0113)  loss_xy_dn_3_unscaled: 0.0068 (0.0113)  loss_xy_dn_4_unscaled: 0.0067 (0.0115)  loss_xy_interm_unscaled: 0.0069 (0.0405)  time: 1.7669  data: 0.0263  max mem: 10858\nEpoch: [0]  [ 60/311]  eta: 0:07:31  lr: 0.000100  class_error: 0.00  loss: 9.8114 (17.8606)  loss_bbox: 0.1126 (0.3623)  loss_bbox_0: 0.0864 (0.3652)  loss_bbox_1: 0.1012 (0.3593)  loss_bbox_2: 0.1118 (0.3615)  loss_bbox_3: 0.1095 (0.3588)  loss_bbox_4: 0.1122 (0.3572)  loss_bbox_dn: 0.0847 (0.2339)  loss_bbox_dn_0: 0.1225 (0.3228)  loss_bbox_dn_1: 0.0874 (0.2215)  loss_bbox_dn_2: 0.0840 (0.2315)  loss_bbox_dn_3: 0.0835 (0.2328)  loss_bbox_dn_4: 0.0844 (0.2324)  loss_bbox_interm: 0.0958 (0.3678)  loss_ce: 0.1192 (0.3132)  loss_ce_0: 0.4045 (0.5433)  loss_ce_1: 0.2738 (0.4752)  loss_ce_2: 0.1899 (0.4011)  loss_ce_3: 0.1405 (0.3684)  loss_ce_4: 0.1281 (0.3269)  loss_ce_dn: 0.0052 (0.1415)  loss_ce_dn_0: 0.1010 (0.3834)  loss_ce_dn_1: 0.0289 (0.2118)  loss_ce_dn_2: 0.0098 (0.1661)  loss_ce_dn_3: 0.0075 (0.1622)  loss_ce_dn_4: 0.0080 (0.1483)  loss_ce_interm: 0.6553 (0.8502)  loss_giou: 0.5308 (0.8190)  loss_giou_0: 0.4554 (0.7794)  loss_giou_1: 0.4852 (0.7794)  loss_giou_2: 0.5158 (0.7996)  loss_giou_3: 0.5141 (0.8026)  loss_giou_4: 0.5312 (0.8137)  loss_giou_dn: 0.4450 (0.6042)  loss_giou_dn_0: 0.5595 (0.7277)  loss_giou_dn_1: 0.4655 (0.6172)  loss_giou_dn_2: 0.4361 (0.6014)  loss_giou_dn_3: 0.4336 (0.6015)  loss_giou_dn_4: 0.4365 (0.6021)  loss_giou_interm: 0.4633 (0.8141)  cardinality_error_unscaled: 0.5000 (0.4959)  cardinality_error_0_unscaled: 0.5000 (0.4959)  cardinality_error_1_unscaled: 0.5000 (0.4959)  cardinality_error_2_unscaled: 0.5000 (0.4959)  cardinality_error_3_unscaled: 0.5000 (0.4959)  cardinality_error_4_unscaled: 0.5000 (0.4959)  cardinality_error_dn_unscaled: 0.5000 (0.4959)  cardinality_error_dn_0_unscaled: 0.5000 (0.4959)  cardinality_error_dn_1_unscaled: 0.5000 (0.4959)  cardinality_error_dn_2_unscaled: 0.5000 (0.4959)  cardinality_error_dn_3_unscaled: 0.5000 (0.4959)  cardinality_error_dn_4_unscaled: 0.5000 (0.4959)  cardinality_error_interm_unscaled: 0.5000 (0.4959)  class_error_unscaled: 0.0000 (0.0000)  loss_bbox_unscaled: 0.0225 (0.0725)  loss_bbox_0_unscaled: 0.0173 (0.0730)  loss_bbox_1_unscaled: 0.0202 (0.0719)  loss_bbox_2_unscaled: 0.0224 (0.0723)  loss_bbox_3_unscaled: 0.0219 (0.0718)  loss_bbox_4_unscaled: 0.0224 (0.0714)  loss_bbox_dn_unscaled: 0.0169 (0.0468)  loss_bbox_dn_0_unscaled: 0.0245 (0.0646)  loss_bbox_dn_1_unscaled: 0.0175 (0.0443)  loss_bbox_dn_2_unscaled: 0.0168 (0.0463)  loss_bbox_dn_3_unscaled: 0.0167 (0.0466)  loss_bbox_dn_4_unscaled: 0.0169 (0.0465)  loss_bbox_interm_unscaled: 0.0192 (0.0736)  loss_ce_unscaled: 0.1192 (0.3132)  loss_ce_0_unscaled: 0.4045 (0.5433)  loss_ce_1_unscaled: 0.2738 (0.4752)  loss_ce_2_unscaled: 0.1899 (0.4011)  loss_ce_3_unscaled: 0.1405 (0.3684)  loss_ce_4_unscaled: 0.1281 (0.3269)  loss_ce_dn_unscaled: 0.0052 (0.1415)  loss_ce_dn_0_unscaled: 0.1010 (0.3834)  loss_ce_dn_1_unscaled: 0.0289 (0.2118)  loss_ce_dn_2_unscaled: 0.0098 (0.1661)  loss_ce_dn_3_unscaled: 0.0075 (0.1622)  loss_ce_dn_4_unscaled: 0.0080 (0.1483)  loss_ce_interm_unscaled: 0.6553 (0.8502)  loss_giou_unscaled: 0.2654 (0.4095)  loss_giou_0_unscaled: 0.2277 (0.3897)  loss_giou_1_unscaled: 0.2426 (0.3897)  loss_giou_2_unscaled: 0.2579 (0.3998)  loss_giou_3_unscaled: 0.2570 (0.4013)  loss_giou_4_unscaled: 0.2656 (0.4069)  loss_giou_dn_unscaled: 0.2225 (0.3021)  loss_giou_dn_0_unscaled: 0.2798 (0.3638)  loss_giou_dn_1_unscaled: 0.2327 (0.3086)  loss_giou_dn_2_unscaled: 0.2180 (0.3007)  loss_giou_dn_3_unscaled: 0.2168 (0.3008)  loss_giou_dn_4_unscaled: 0.2183 (0.3011)  loss_giou_interm_unscaled: 0.2317 (0.4070)  loss_hw_unscaled: 0.0157 (0.0416)  loss_hw_0_unscaled: 0.0116 (0.0388)  loss_hw_1_unscaled: 0.0145 (0.0397)  loss_hw_2_unscaled: 0.0154 (0.0400)  loss_hw_3_unscaled: 0.0151 (0.0392)  loss_hw_4_unscaled: 0.0157 (0.0400)  loss_hw_dn_unscaled: 0.0117 (0.0361)  loss_hw_dn_0_unscaled: 0.0164 (0.0519)  loss_hw_dn_1_unscaled: 0.0120 (0.0336)  loss_hw_dn_2_unscaled: 0.0115 (0.0358)  loss_hw_dn_3_unscaled: 0.0114 (0.0361)  loss_hw_dn_4_unscaled: 0.0117 (0.0359)  loss_hw_interm_unscaled: 0.0120 (0.0386)  loss_xy_unscaled: 0.0074 (0.0309)  loss_xy_0_unscaled: 0.0054 (0.0342)  loss_xy_1_unscaled: 0.0058 (0.0322)  loss_xy_2_unscaled: 0.0068 (0.0323)  loss_xy_3_unscaled: 0.0072 (0.0325)  loss_xy_4_unscaled: 0.0072 (0.0314)  loss_xy_dn_unscaled: 0.0057 (0.0107)  loss_xy_dn_0_unscaled: 0.0081 (0.0127)  loss_xy_dn_1_unscaled: 0.0062 (0.0107)  loss_xy_dn_2_unscaled: 0.0057 (0.0105)  loss_xy_dn_3_unscaled: 0.0055 (0.0105)  loss_xy_dn_4_unscaled: 0.0055 (0.0106)  loss_xy_interm_unscaled: 0.0058 (0.0349)  time: 1.7896  data: 0.0273  max mem: 10858\nEpoch: [0]  [ 70/311]  eta: 0:07:13  lr: 0.000100  class_error: 0.00  loss: 8.8882 (16.6091)  loss_bbox: 0.1138 (0.3264)  loss_bbox_0: 0.0891 (0.3261)  loss_bbox_1: 0.1017 (0.3231)  loss_bbox_2: 0.1053 (0.3249)  loss_bbox_3: 0.1084 (0.3231)  loss_bbox_4: 0.1132 (0.3220)  loss_bbox_dn: 0.0906 (0.2133)  loss_bbox_dn_0: 0.1189 (0.2941)  loss_bbox_dn_1: 0.0881 (0.2032)  loss_bbox_dn_2: 0.0883 (0.2113)  loss_bbox_dn_3: 0.0890 (0.2123)  loss_bbox_dn_4: 0.0872 (0.2119)  loss_bbox_interm: 0.0958 (0.3286)  loss_ce: 0.1037 (0.2829)  loss_ce_0: 0.3591 (0.5147)  loss_ce_1: 0.2393 (0.4390)  loss_ce_2: 0.1721 (0.3655)  loss_ce_3: 0.1203 (0.3318)  loss_ce_4: 0.1102 (0.2946)  loss_ce_dn: 0.0025 (0.1224)  loss_ce_dn_0: 0.0886 (0.3409)  loss_ce_dn_1: 0.0197 (0.1846)  loss_ce_dn_2: 0.0065 (0.1438)  loss_ce_dn_3: 0.0042 (0.1403)  loss_ce_dn_4: 0.0047 (0.1286)  loss_ce_interm: 0.5603 (0.8007)  loss_giou: 0.5293 (0.7760)  loss_giou_0: 0.4189 (0.7299)  loss_giou_1: 0.4852 (0.7391)  loss_giou_2: 0.5043 (0.7568)  loss_giou_3: 0.5104 (0.7615)  loss_giou_4: 0.5308 (0.7714)  loss_giou_dn: 0.4157 (0.5797)  loss_giou_dn_0: 0.5324 (0.6999)  loss_giou_dn_1: 0.4655 (0.5934)  loss_giou_dn_2: 0.4168 (0.5771)  loss_giou_dn_3: 0.4108 (0.5772)  loss_giou_dn_4: 0.4149 (0.5772)  loss_giou_interm: 0.4669 (0.7601)  cardinality_error_unscaled: 0.5000 (0.4930)  cardinality_error_0_unscaled: 0.5000 (0.4930)  cardinality_error_1_unscaled: 0.5000 (0.4930)  cardinality_error_2_unscaled: 0.5000 (0.4930)  cardinality_error_3_unscaled: 0.5000 (0.4930)  cardinality_error_4_unscaled: 0.5000 (0.4930)  cardinality_error_dn_unscaled: 0.5000 (0.4930)  cardinality_error_dn_0_unscaled: 0.5000 (0.4930)  cardinality_error_dn_1_unscaled: 0.5000 (0.4930)  cardinality_error_dn_2_unscaled: 0.5000 (0.4930)  cardinality_error_dn_3_unscaled: 0.5000 (0.4930)  cardinality_error_dn_4_unscaled: 0.5000 (0.4930)  cardinality_error_interm_unscaled: 0.5000 (0.4930)  class_error_unscaled: 0.0000 (0.7042)  loss_bbox_unscaled: 0.0228 (0.0653)  loss_bbox_0_unscaled: 0.0178 (0.0652)  loss_bbox_1_unscaled: 0.0203 (0.0646)  loss_bbox_2_unscaled: 0.0211 (0.0650)  loss_bbox_3_unscaled: 0.0217 (0.0646)  loss_bbox_4_unscaled: 0.0226 (0.0644)  loss_bbox_dn_unscaled: 0.0181 (0.0427)  loss_bbox_dn_0_unscaled: 0.0238 (0.0588)  loss_bbox_dn_1_unscaled: 0.0176 (0.0406)  loss_bbox_dn_2_unscaled: 0.0177 (0.0423)  loss_bbox_dn_3_unscaled: 0.0178 (0.0425)  loss_bbox_dn_4_unscaled: 0.0174 (0.0424)  loss_bbox_interm_unscaled: 0.0192 (0.0657)  loss_ce_unscaled: 0.1037 (0.2829)  loss_ce_0_unscaled: 0.3591 (0.5147)  loss_ce_1_unscaled: 0.2393 (0.4390)  loss_ce_2_unscaled: 0.1721 (0.3655)  loss_ce_3_unscaled: 0.1203 (0.3318)  loss_ce_4_unscaled: 0.1102 (0.2946)  loss_ce_dn_unscaled: 0.0025 (0.1224)  loss_ce_dn_0_unscaled: 0.0886 (0.3409)  loss_ce_dn_1_unscaled: 0.0197 (0.1846)  loss_ce_dn_2_unscaled: 0.0065 (0.1438)  loss_ce_dn_3_unscaled: 0.0042 (0.1403)  loss_ce_dn_4_unscaled: 0.0047 (0.1286)  loss_ce_interm_unscaled: 0.5603 (0.8007)  loss_giou_unscaled: 0.2647 (0.3880)  loss_giou_0_unscaled: 0.2095 (0.3650)  loss_giou_1_unscaled: 0.2426 (0.3696)  loss_giou_2_unscaled: 0.2521 (0.3784)  loss_giou_3_unscaled: 0.2552 (0.3807)  loss_giou_4_unscaled: 0.2654 (0.3857)  loss_giou_dn_unscaled: 0.2079 (0.2898)  loss_giou_dn_0_unscaled: 0.2662 (0.3500)  loss_giou_dn_1_unscaled: 0.2327 (0.2967)  loss_giou_dn_2_unscaled: 0.2084 (0.2886)  loss_giou_dn_3_unscaled: 0.2054 (0.2886)  loss_giou_dn_4_unscaled: 0.2074 (0.2886)  loss_giou_interm_unscaled: 0.2334 (0.3801)  loss_hw_unscaled: 0.0150 (0.0377)  loss_hw_0_unscaled: 0.0116 (0.0349)  loss_hw_1_unscaled: 0.0136 (0.0359)  loss_hw_2_unscaled: 0.0142 (0.0362)  loss_hw_3_unscaled: 0.0147 (0.0356)  loss_hw_4_unscaled: 0.0148 (0.0363)  loss_hw_dn_unscaled: 0.0112 (0.0325)  loss_hw_dn_0_unscaled: 0.0158 (0.0467)  loss_hw_dn_1_unscaled: 0.0118 (0.0304)  loss_hw_dn_2_unscaled: 0.0111 (0.0323)  loss_hw_dn_3_unscaled: 0.0112 (0.0326)  loss_hw_dn_4_unscaled: 0.0111 (0.0323)  loss_hw_interm_unscaled: 0.0131 (0.0349)  loss_xy_unscaled: 0.0075 (0.0276)  loss_xy_0_unscaled: 0.0060 (0.0303)  loss_xy_1_unscaled: 0.0061 (0.0287)  loss_xy_2_unscaled: 0.0072 (0.0288)  loss_xy_3_unscaled: 0.0073 (0.0290)  loss_xy_4_unscaled: 0.0075 (0.0281)  loss_xy_dn_unscaled: 0.0066 (0.0101)  loss_xy_dn_0_unscaled: 0.0081 (0.0121)  loss_xy_dn_1_unscaled: 0.0066 (0.0102)  loss_xy_dn_2_unscaled: 0.0066 (0.0099)  loss_xy_dn_3_unscaled: 0.0064 (0.0099)  loss_xy_dn_4_unscaled: 0.0065 (0.0100)  loss_xy_interm_unscaled: 0.0060 (0.0308)  time: 1.7919  data: 0.0265  max mem: 10858\n^C\nW1209 12:42:32.941000 256 torch/distributed/elastic/agent/server/api.py:719] Received 2 death signal, shutting down workers\nW1209 12:42:32.942000 256 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 259 closing signal SIGINT\nW1209 12:42:32.942000 256 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 260 closing signal SIGINT\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"id":"6PHSfq59wEGL","trusted":true},"outputs":[],"execution_count":null}]}